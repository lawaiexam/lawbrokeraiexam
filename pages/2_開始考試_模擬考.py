import time
import pandas as pd
import streamlit as st
import random
import re

from services.state_service import ensure_state
from services.auth_service import require_login_or_render
from services.bank_service import load_bank_df
from services.exam_service import grade_paper, persist_exam_record
from services.exam_rules import CERT_CATALOG
from components.auth_ui import render_user_panel
from components.sidebar_exam_settings import render_exam_settings
from components.question_render import render_question

# ==========================================
# ğŸŸ¢ è¨­å®šå€ï¼šæ–°ç‰ˆå‡ºé¡Œæ¬Šé‡èˆ‡ç« ç¯€æ˜ å°„ (æºè‡ª percentage.json)
# ==========================================

# 1. è€ƒé¡Œçµæ§‹å®šç¾© (NEW_EXAM_WEIGHTS)
# é€™è£¡å®šç¾©äº†ï¼šè­‰ç…§ -> ç§‘ç›® (Subject) -> ç« ç¯€ (Chapter) çš„æ¬Šé‡
NEW_EXAM_WEIGHTS = {
    "äººèº«ä¿éšªæ¥­å‹™å“¡è³‡æ ¼æ¸¬é©—": {
        # ç¬¬ä¸€ç¯€ï¼šä¿éšªæ³•è¦
        "life_regulation": {
            "insurance_law_core": 40,   # ä¿éšªæ³•ï¼ˆç¸½å‰‡ï¼å¥‘ç´„ï¼åŸºæœ¬è¦ç¯„ï¼‰
            "solicitation_rules": 40,   # æ‹›æ”¬è¡Œç‚ºèˆ‡æ¥­å‹™å“¡ç®¡ç†è¦ç¯„
            "liability_penalties": 20   # è²¬ä»»æ­¸å±¬èˆ‡ç½°å‰‡
        },
        # ç¬¬äºŒç¯€ï¼šä¿éšªå¯¦å‹™
        "life_practice": {
            "insurance_principles": 30, # ä¿éšªå­¸åŸç†èˆ‡é¢¨éšªæ¦‚å¿µ
            "life_products": 50,        # äººèº«ä¿éšªå•†å“
            "sales_practice_ethics": 20 # æ‹›æ”¬å¯¦å‹™èˆ‡å€«ç†
        }
    },
    "å¤–å¹£æ”¶ä»˜éæŠ•è³‡å‹ä¿éšªå•†å“æ¸¬é©—": {
        # å–®ä¸€ç§‘ç›®
        "fx_exam": {
            "fx_basics": 28,                # å¤–åŒ¯èˆ‡åŒ¯ç‡åŸºç¤
            "fx_products": 28,              # å¤–å¹£éæŠ•è³‡å‹å•†å“èˆ‡äº¤æ˜“æµç¨‹
            "fx_regulation_compliance": 22, # æ³•ä»¤è¦ç¯„èˆ‡éµå¾ª
            "fx_risk_disclosure_practice": 22 # é¢¨éšªæ­éœ²èˆ‡éŠ·å”®å¯¦å‹™
        }
    },
    "æŠ•è³‡å‹ä¿éšªå•†å“æ¥­å‹™å“¡æ¸¬é©—": {
        # ç¬¬ä¸€ç¯€ï¼šæ³•ä»¤è¦ç«  (æ³¨æ„ï¼šå¯¦éš›è€ƒè©¦é †åºå¯èƒ½ä¸åŒï¼Œé€™è£¡ä»¥ Subject ID ç‚ºæº–)
        "il_regulations": {
            "sales_regulations": 50,    # éŠ·å”®è¦ç¯„èˆ‡è³‡è¨Šæ­éœ²
            "suitability_rules": 30,    # é©åˆåº¦è¦ç¯„
            "dispute_liability": 20     # è²¬ä»»èˆ‡çˆ­è­°è™•ç†
        },
        # ç¬¬äºŒç¯€ï¼šæŠ•è³‡å¯¦å‹™
        "il_investment_practice": {
            "investment_basics": 45,        # æŠ•è³‡å·¥å…·èˆ‡é¢¨éšªå ±é…¬
            "il_product_mechanics": 45,     # æŠ•è³‡å‹å•†å“æ©Ÿåˆ¶
            "customer_suitability_practice": 10 # å®¢æˆ¶é©åˆåº¦èˆ‡éŠ·å”®æµç¨‹å¯¦å‹™
        }
    }
}

# 2. ç§‘ç›®è­˜åˆ¥æ˜ å°„ (Subject Mapping)
# é€éæ¨¡æ“¬è€ƒè¨­å®šçš„ã€Œç¯€æ¬¡åç¨±ã€ä¾†å°‹æ‰¾å°æ‡‰çš„ã€ŒSubject IDã€
# é—œéµå­—æ¯”å°ï¼šåªè¦ç¯€æ¬¡åç¨±åŒ…å« key ä¸­çš„æ–‡å­—ï¼Œå°±è¦–ç‚ºè©²ç§‘ç›®
SUBJECT_IDENTIFIER = {
    "äººèº«ä¿éšªæ¥­å‹™å“¡è³‡æ ¼æ¸¬é©—": {
        "æ³•è¦": "life_regulation",
        "å¯¦å‹™": "life_practice"
    },
    "å¤–å¹£æ”¶ä»˜éæŠ•è³‡å‹ä¿éšªå•†å“æ¸¬é©—": {
        "å¤–å¹£": "fx_exam",
        "éæŠ•è³‡": "fx_exam"
    },
    "æŠ•è³‡å‹ä¿éšªå•†å“æ¥­å‹™å“¡æ¸¬é©—": {
        "æ³•ä»¤": "il_regulations",
        "è¦ç« ": "il_regulations",
        "ç¬¬ä¸€ç¯€": "il_regulations", # å‡è¨­ç¬¬ä¸€ç¯€æ˜¯è€ƒæ³•è¦
        "å¯¦å‹™": "il_investment_practice",
        "ç¬¬äºŒç¯€": "il_investment_practice" # å‡è¨­ç¬¬äºŒç¯€æ˜¯è€ƒå¯¦å‹™
    }
}

# 3. ç« ç¯€æ­¸é¡æ˜ å°„ (Chapter Mapping)
# å°‡ AI åˆ†é¡çš„ã€Œä¸­æ–‡ç« ç¯€åç¨±ã€æ­¸é¡åˆ° JSON å®šç¾©çš„ã€ŒChapter IDã€
CHAPTER_MAPPING = {
    # === äººèº«ä¿éšª ===
    "ä¿éšªæ³•è¦": { # å°æ‡‰ Subject ID: life_regulation
        "ä¿éšªå¥‘ç´„": "insurance_law_core",
        "ä¿éšªå¥‘ç´„å…­å¤§åŸå‰‡": "insurance_law_core",
        "å¥‘ç´„è§£é™¤ã€ç„¡æ•ˆã€å¤±æ•ˆã€åœæ•ˆã€å¾©æ•ˆ": "insurance_law_core",
        "ä¿éšªé‡‘èˆ‡è§£ç´„é‡‘": "insurance_law_core",
        "éºç”¢ç¨…ã€è´ˆèˆ‡ç¨…": "insurance_law_core",
        "æ‰€å¾—ç¨…": "insurance_law_core",
        "é‡‘èæ¶ˆè²»è€…ä¿è­·æ³•": "insurance_law_core",
        "å€‹äººè³‡æ–™ä¿è­·æ³•": "insurance_law_core",
        "æ´—éŒ¢é˜²åˆ¶æ³•": "liability_penalties", 
        "ä¿éšªæ¥­å‹™å“¡ç›¸é—œæ³•è¦åŠè¦å®š": "solicitation_rules"
    },
    "ä¿éšªå¯¦å‹™": { # å°æ‡‰ Subject ID: life_practice
        "é¢¨éšªèˆ‡é¢¨éšªç®¡ç†": "insurance_principles",
        "äººèº«ä¿éšªæ­·å²åŠç”Ÿå‘½è¡¨": "insurance_principles",
        "ä¿éšªè²»æ¶æ§‹ã€è§£ç´„é‡‘ã€æº–å‚™é‡‘ã€ä¿å–®ç´…åˆ©": "insurance_principles",
        "ä¿éšªä¸­é‡è¦çš„è§’è‰²": "insurance_principles",
        "äººèº«ä¿éšªæ„ç¾©ã€åŠŸèƒ½ã€åˆ†é¡": "life_products",
        "äººèº«ä¿éšªï¼äººå£½ä¿éšª": "life_products",
        "äººèº«ä¿éšªï¼å¹´é‡‘ä¿éšª": "life_products",
        "äººèº«ä¿éšªï¼å¥åº·ä¿éšª": "life_products",
        "äººèº«ä¿éšªï¼å‚·å®³ä¿éšª": "life_products",
        "äººèº«ä¿éšªï¼å…¶ä»–äººèº«ä¿éšª": "life_products",
        "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·": "sales_practice_ethics",
        "ç¹¼æ‰¿ç›¸é—œ": "sales_practice_ethics"
    },

    # === å¤–å¹£ä¿å–® ===
    "å¤–å¹£éæŠ•è³‡å‹": { # å°æ‡‰ Subject ID: fx_exam
        "å£½éšªåŸºæœ¬æ¦‚å¿µ": "fx_basics",
        "äººèº«ä¿éšªæ¥­è¾¦ç†ä»¥å¤–å¹£æ”¶ä»˜ä¹‹éæŠ•è³‡å‹äººèº«ä¿éšªæ¥­å‹™æ‡‰å…·å‚™è³‡æ ¼æ¢ä»¶åŠæ³¨æ„äº‹é …": "fx_products",
        "ä¿éšªæ¥­è¾¦ç†å¤–åŒ¯æ¥­å‹™ç®¡ç†è¾¦æ³•": "fx_regulation_compliance",
        "ç®¡ç†å¤–åŒ¯æ¢ä¾‹": "fx_regulation_compliance",
        "å¤–åŒ¯æ”¶æ”¯æˆ–äº¤æ˜“ç”³å ±è¾¦æ³•": "fx_regulation_compliance",
        "ä¿éšªæ¥­è¾¦ç†åœ‹å¤–æŠ•è³‡ç®¡ç†è¾¦æ³•": "fx_regulation_compliance",
        "ä¿éšªæ¥­å„é¡ç›£æ§æªæ–½": "fx_regulation_compliance",
        "éŠ·å”®æ‡‰æ³¨æ„äº‹é …": "fx_risk_disclosure_practice",
        "æ–°å‹æ…‹äººèº«ä¿éšªå•†å“å¯©æŸ¥": "fx_risk_disclosure_practice",
        "æŠ•è³‡å‹ä¿éšªå°ˆè¨­å¸³ç°¿ä¿ç®¡æ©Ÿæ§‹åŠæŠ•è³‡æ¨™çš„æ‡‰æ³¨æ„äº‹é …": "fx_risk_disclosure_practice",
        "æŠ•è³‡å‹ä¿éšªè§€å¿µ": "fx_products" # æ­¸é¡åˆ°ç”¢å“
    },

    # === æŠ•è³‡å‹ä¿éšª ===
    "æŠ•è³‡å‹æ³•è¦": { # å°æ‡‰ Subject ID: il_regulations
        "æŠ•è³‡å‹ä¿éšªæ³•ä»¤ä»‹ç´¹": "sales_regulations",
        "è­‰åˆ¸æŠ•è³‡ä¿¡è¨—åŠé¡§å•ä¹‹è¦ç¯„èˆ‡åˆ¶åº¦": "sales_regulations",
        "éŠ·å”®æ‡‰æ³¨æ„äº‹é …": "sales_regulations",
        # è‹¥æœ‰ AI åˆ†é¡åˆ°é€™é¡ï¼Œæ˜ å°„åˆ°é©åˆåº¦
        "é©åˆåº¦": "suitability_rules",
        "çˆ­è­°è™•ç†": "dispute_liability"
    },
    "æŠ•è³‡å‹å¯¦å‹™": { # å°æ‡‰ Subject ID: il_investment_practice
        "è²¨å¹£æ™‚é–“åƒ¹å€¼": "investment_basics",
        "å‚µåˆ¸è©•åƒ¹": "investment_basics",
        "è­‰åˆ¸è©•åƒ¹": "investment_basics",
        "é¢¨éšªã€å ±é…¬èˆ‡æŠ•è³‡çµ„åˆ": "investment_basics",
        "è³‡æœ¬è³‡ç”¢è¨‚åƒ¹æ¨¡å¼ã€ç¸¾æ•ˆ": "investment_basics",
        "æŠ•è³‡å·¥å…·ç°¡ä»‹": "investment_basics",
        "é‡‘èé«”ç³»æ¦‚è¿°": "investment_basics",
        "æŠ•è³‡å‹ä¿éšªæ¦‚è«–": "il_product_mechanics",
        "æŠ•è³‡å‹ä¿éšªè§€å¿µ": "il_product_mechanics",
        "æŠ•è³‡å‹ä¿éšªå°ˆè¨­å¸³ç°¿ä¿ç®¡æ©Ÿæ§‹åŠæŠ•è³‡æ¨™çš„æ‡‰æ³¨æ„äº‹é …": "il_product_mechanics",
        # å¯¦å‹™ä¸Šçš„éŠ·å”®æµç¨‹
        "å®¢æˆ¶é©åˆåº¦": "customer_suitability_practice"
    }
}

# ==========================================
# ğŸŸ¢ æ ¸å¿ƒå‡½å¼ï¼šæ¬Šé‡åŒ–æŠ½é¡Œ (Advanced)
# ==========================================
def build_weighted_paper_v2(full_df, cert_type, section_name, total_questions, shuffle_options=False):
    """
    æ ¹æ“šæ–°ç‰ˆ JSON é‚è¼¯é€²è¡ŒæŠ½é¡Œã€‚
    1. è­˜åˆ¥ç•¶å‰è€ƒç§‘ (Subject)ã€‚
    2. å–å¾—è©²è€ƒç§‘çš„ç« ç¯€æ¬Šé‡ã€‚
    3. å°‡ AI åˆ†é¡æ˜ å°„åˆ° JSON ç« ç¯€ IDã€‚
    4. åŸ·è¡ŒåŠ æ¬ŠæŠ½æ¨£ã€‚
    """
    target_col = "AIåˆ†é¡ç« ç¯€"
    if full_df.empty or target_col not in full_df.columns:
        return full_df.sample(n=min(len(full_df), total_questions)).to_dict('records')

    # 1. è­˜åˆ¥ Subject ID
    subject_id = None
    cert_identifiers = SUBJECT_IDENTIFIER.get(cert_type, {})
    
    # å˜—è©¦ç”¨ç¯€æ¬¡åç¨±ä¾†åŒ¹é… (ä¾‹å¦‚ "ç¬¬ä¸€ç¯€ï¼šä¿éšªæ³•è¦" -> åŒ¹é… "æ³•è¦" -> "life_regulation")
    for keyword, sid in cert_identifiers.items():
        if keyword in section_name:
            subject_id = sid
            break
            
    # å¦‚æœæ‰¾ä¸åˆ°å°æ‡‰çš„ Subjectï¼Œé€€å›è‡ªç„¶åˆ†ä½ˆæŠ½æ¨£
    if not subject_id:
        print(f"Warning: Could not identify subject for section '{section_name}' in cert '{cert_type}'. Using standard distribution.")
        return _build_paper_by_natural_distribution(full_df, total_questions)

    # 2. å–å¾—è©² Subject çš„æ¬Šé‡è¨­å®š
    cert_weights = NEW_EXAM_WEIGHTS.get(cert_type, {})
    chapter_weights = cert_weights.get(subject_id, {})
    
    if not chapter_weights:
        return _build_paper_by_natural_distribution(full_df, total_questions)

    # 3. å»ºç«‹æ˜ å°„è¡¨ (AI Chapter -> JSON Chapter ID)
    # ç‚ºäº†ç°¡åŒ–ï¼Œæˆ‘å€‘å°‡ CHAPTER_MAPPING æ‰å¹³åŒ–æœå°‹ï¼Œæˆ–å»ºç«‹ä¸€å€‹è‡¨æ™‚çš„å¤§è¡¨
    # é€™è£¡æ¡ç”¨ç°¡å–®ç­–ç•¥ï¼šæ ¹æ“š subject_id æ‰¾å°æ‡‰çš„ mapping key
    mapping_key_map = {
        "life_regulation": "ä¿éšªæ³•è¦",
        "life_practice": "ä¿éšªå¯¦å‹™",
        "fx_exam": "å¤–å¹£éæŠ•è³‡å‹",
        "il_regulations": "æŠ•è³‡å‹æ³•è¦",
        "il_investment_practice": "æŠ•è³‡å‹å¯¦å‹™"
    }
    mapping_category = mapping_key_map.get(subject_id)
    current_mapping = CHAPTER_MAPPING.get(mapping_category, {})

    # 4. ç‚º DataFrame æ¨™è¨˜ JSON Chapter ID
    # å¦‚æœæ‰¾ä¸åˆ°æ˜ å°„ï¼Œæ¨™è¨˜ç‚º "others"
    df_temp = full_df.copy()
    df_temp["JsonChapterID"] = df_temp[target_col].map(current_mapping).fillna("others")

    # 5. è¨ˆç®—å„ç« ç¯€ç›®æ¨™é¡Œæ•¸
    exam_pool = []
    
    # éæ­·æ¬Šé‡è¨­å®š (ä¾‹å¦‚ insurance_law_core: 40%)
    for ch_id, weight_pct in chapter_weights.items():
        target_count = int(round(total_questions * (weight_pct / 100)))
        
        # å¾ df_temp ä¸­æ‰¾å‡ºå±¬æ–¼é€™å€‹ ch_id çš„é¡Œç›®
        # æ³¨æ„ï¼šå¤šå€‹ AI ç« ç¯€å¯èƒ½å°æ‡‰åˆ°åŒä¸€å€‹ ch_id
        chapter_pool = df_temp[df_temp["JsonChapterID"] == ch_id]
        
        available = len(chapter_pool)
        take_n = min(available, target_count)
        
        if take_n > 0:
            selected = chapter_pool.sample(n=take_n)
            exam_pool.append(selected)

    # 6. è£œé¡Œæ©Ÿåˆ¶ (è™•ç† "others" æˆ– å››æ¨äº”å…¥é€ æˆçš„ä¸è¶³)
    current_selected = pd.concat(exam_pool) if exam_pool else pd.DataFrame()
    needed = total_questions - len(current_selected)
    
    if needed > 0:
        # å„ªå…ˆå¾ "others" (æœªæ­¸é¡ä½†å±¬æ–¼æœ¬æª”çš„é¡Œç›®) æŠ½
        others_pool = df_temp[~df_temp.index.isin(current_selected.index)]
        if not others_pool.empty:
            extra = others_pool.sample(n=min(len(others_pool), needed))
            exam_pool.append(extra)
            
    # åˆä½µ
    if exam_pool:
        final_df = pd.concat(exam_pool)
    else:
        final_df = pd.DataFrame()

    # 7. ç¸½æ•¸æ§åˆ¶
    if len(final_df) > total_questions:
        final_df = final_df.sample(n=total_questions)

    # 8. æ‰“äº‚
    final_df = final_df.sample(frac=1).reset_index(drop=True)
    return final_df.to_dict('records')

def _build_paper_by_natural_distribution(full_df, total_questions):
    """å‚™ç”¨ï¼šä¾é¡Œåº«è‡ªç„¶åˆ†ä½ˆæŠ½æ¨£"""
    target_col = "AIåˆ†é¡ç« ç¯€"
    if target_col not in full_df.columns:
        return full_df.sample(n=min(len(full_df), total_questions)).to_dict('records')
        
    valid_df = full_df[full_df[target_col].notna()]
    if valid_df.empty: 
        return full_df.sample(n=min(len(full_df), total_questions)).to_dict('records')

    chapter_counts = valid_df[target_col].value_counts()
    total_bank_size = len(valid_df)
    exam_pool = []
    
    for chapter, count in chapter_counts.items():
        ratio = count / total_bank_size
        n_for_chapter = int(round(total_questions * ratio))
        if n_for_chapter == 0 and count > 0: n_for_chapter = 1
        chapter_df = valid_df[valid_df[target_col] == chapter]
        exam_pool.append(chapter_df.sample(n=min(len(chapter_df), n_for_chapter)))

    exam_df = pd.concat(exam_pool) if exam_pool else pd.DataFrame()
    
    if len(exam_df) < total_questions:
        rem = valid_df[~valid_df.index.isin(exam_df.index)]
        if not rem.empty:
            exam_df = pd.concat([exam_df, rem.sample(n=min(len(rem), total_questions - len(exam_df)))])
            
    if len(exam_df) > total_questions:
        exam_df = exam_df.sample(n=total_questions)
        
    return exam_df.sample(frac=1).reset_index(drop=True).to_dict('records')

# ==========================================
# ä¸»ç¨‹å¼é–‹å§‹
# ==========================================

ensure_state()

with st.sidebar:
    render_user_panel()

user = require_login_or_render()
if user is None: st.stop()

st.title("é–‹å§‹è€ƒè©¦ - æ¨¡æ“¬è€ƒ")

with st.sidebar:
    settings = render_exam_settings(mode="mock")

spec = settings.get("mock_spec") or {}
sections = spec.get("sections") or []
if not sections:
    st.error("æ­¤è­‰ç…§é¡åˆ¥æ²’æœ‰è¨­å®šæ¨¡æ“¬è€ƒè¦å‰‡ï¼ˆMOCK_SPECSï¼‰ã€‚")
    st.stop()

# ========= åˆå§‹åŒ–ç‹€æ…‹ =========
if "mock_section_idx" not in st.session_state: st.session_state.mock_section_idx = 0
if "mock_section_results" not in st.session_state: st.session_state.mock_section_results = []
if "mock_exam_start_ts" not in st.session_state: st.session_state.mock_exam_start_ts = None

# ========= å–å¾—ç›®å‰ç¯€æ¬¡ =========
sec_idx = int(st.session_state.mock_section_idx)
if sec_idx >= len(sections):
    st.session_state.mock_section_idx = 0
    st.session_state.mock_section_results = []
    st.session_state.mock_exam_start_ts = None
    sec_idx = 0

section = sections[sec_idx]
section_name = section.get("name", f"Section{sec_idx+1}")
n_questions = int(section.get("n_questions", 0))
time_limit_sec = int(section.get("time_min", 0) * 60)

if n_questions <= 0:
    st.error("æ¨¡æ“¬è€ƒè¦å‰‡è¨­å®šä¸å®Œæ•´ï¼Œè«‹æª¢æŸ¥ MOCK_SPECSã€‚")
    st.stop()

# ========= è¼‰å…¥æœ¬ç¯€é¡Œåº« =========
try:
    bank_path = CERT_CATALOG[settings["cert_type"]]["subjects"][section_name]
except Exception:
    st.error(f"æ‰¾ä¸åˆ°é¡Œåº«æ˜ å°„ï¼š{settings['cert_type']} â†’ {section_name}")
    st.stop()

df = load_bank_df(settings.get("cert_type", ""), merge_all=False, bank_source_path=bank_path)

if df is None or df.empty:
    st.warning("å°šæœªè¼‰å…¥é¡Œåº«ï¼Œè«‹ç¢ºèªé¡Œåº«æª”æ¡ˆæ˜¯å¦å­˜åœ¨ã€‚")
    st.stop()

st.session_state.df = df
filtered = df
exam_label = f"{settings['cert_type']}ï½œæ¨¡æ“¬è€ƒ"
st.session_state.current_bank_name = exam_label

# ========= é¡¯ç¤ºè¦æ ¼ =========
with st.expander("æœ¬æ¬¡æ¨¡æ“¬è€ƒè¦æ ¼", expanded=True):
    st.write(f"- é¡åˆ¥ï¼š{settings['cert_type']}")
    st.write(f"- æ¨¡å¼ï¼š{'å…©ç¯€é€£è€ƒ' if len(sections) > 1 else 'å–®ç¯€'}")
    
    # è­˜åˆ¥ç•¶å‰æ¬Šé‡è¨­å®š
    subject_id = None
    for kw, sid in SUBJECT_IDENTIFIER.get(settings["cert_type"], {}).items():
        if kw in section_name:
            subject_id = sid
            break
            
    if subject_id:
        weights = NEW_EXAM_WEIGHTS[settings["cert_type"]].get(subject_id, {})
        st.info(f"ğŸ’¡ æœ¬ç¯€ ({section_name}) æ¡ç”¨æ¬Šé‡æŠ½æ¨£ï¼š\n" + ", ".join([f"{k}:{v}%" for k,v in weights.items()]))
    else:
        st.write("ğŸ’¡ æœ¬ç¯€æ¡ç”¨è‡ªç„¶åˆ†ä½ˆæŠ½æ¨£")

    st.write("")
    for i, s in enumerate(sections, start=1):
        st.write(f"- ç¬¬ {i} ç¯€ï¼š{s['name']}ï½œ{s['n_questions']} é¡Œï½œ{s['time_min']} åˆ†é˜")

st.divider()
st.subheader(f"ç¬¬ {sec_idx+1} ç¯€ï¼š{section_name}")

# ========= æ§åˆ¶æŒ‰éˆ• =========
colA, colB = st.columns([1, 1])

def _reset_whole_mock_exam():
    for k in ["paper", "answers", "started", "show_results", "saved_to_db", "start_ts", "time_limit"]:
        if k in st.session_state: del st.session_state[k]
    st.session_state.mock_section_idx = 0
    st.session_state.mock_section_results = []
    st.session_state.mock_exam_start_ts = None
    for k in ["mock_summary", "score_tuple", "wrong_df", "results_df", "section_scores", "total_score", "passed", "fail_reason"]:
        if k in st.session_state: del st.session_state[k]

with colA:
    if st.button("é–‹å§‹æœ¬ç¯€", type="primary"):
        # ğŸŸ¢ å‘¼å« V2 ç‰ˆæ¬Šé‡æŠ½é¡Œ
        st.session_state.paper = build_weighted_paper_v2(
            filtered,
            settings["cert_type"],
            section_name, # å‚³å…¥ç¯€æ¬¡åç¨±ä»¥è­˜åˆ¥ Subject
            n_questions,
            shuffle_options=settings["shuffle_options"]
        )
        
        st.session_state.answers = {}
        st.session_state.started = True
        st.session_state.show_results = False
        st.session_state.saved_to_db = False
        st.session_state.start_ts = time.time()
        if st.session_state.mock_exam_start_ts is None:
            st.session_state.mock_exam_start_ts = st.session_state.start_ts
        st.session_state.time_limit = time_limit_sec
        st.rerun()

with colB:
    if st.button("é‡ç½®æ•´å ´æ¨¡æ“¬è€ƒ", type="secondary"):
        _reset_whole_mock_exam()
        st.rerun()

paper = st.session_state.get("paper")
if not paper:
    st.info("è«‹å…ˆæŒ‰ã€Œé–‹å§‹æœ¬ç¯€ã€ã€‚")
    st.stop()

# ========= Timer =========
if st.session_state.get("time_limit") and st.session_state.get("start_ts"):
    elapsed = int(time.time() - st.session_state.start_ts)
    remain = max(0, st.session_state.time_limit - elapsed)
    mins, secs = divmod(remain, 60)
    st.metric("æœ¬ç¯€å‰©é¤˜æ™‚é–“", f"{mins} åˆ† {secs:02d} ç§’")

    if remain == 0 and not st.session_state.get("show_results"):
        st.warning("æ™‚é–“åˆ°ï¼Œè‡ªå‹•äº¤å·ã€‚")
        st.session_state.show_results = True
        st.rerun()

# ========= ä½œç­”å€ =========
if not st.session_state.get("show_results"):
    st.subheader("ä½œç­”å€")
    for idx, q in enumerate(paper, start=1):
        with st.expander(f"ç¬¬ {idx} é¡Œ", expanded=(idx == 1)):
            picked = render_question(q, show_image=settings["show_image"], answer_key=f"mock_s{sec_idx}_ans_{q['ID']}")
            st.session_state.answers[q["ID"]] = picked

    if st.button("äº¤å·ï¼ˆæœ¬ç¯€ï¼‰", type="primary"):
        st.session_state.show_results = True
        st.rerun()

# ========= äº¤å·å¾Œè™•ç† =========
if not st.session_state.get("show_results"): st.stop()

results_df, score_tuple, wrong_df = grade_paper(paper, st.session_state.answers)
correct, total, score = score_tuple

st.session_state.mock_section_results.append({
    "section": section_name,
    "score": int(score),
    "correct": int(correct),
    "total": int(total),
    "results_df": results_df,
    "wrong_df": wrong_df,
})

st.session_state.mock_section_idx += 1

if st.session_state.mock_section_idx < len(sections):
    st.success(f"å·²å®Œæˆç¬¬ {sec_idx+1} ç¯€ï¼š{section_name}ï¼ˆ{score} åˆ†ï¼‰ã€‚")
    st.session_state.paper = None
    st.session_state.answers = {}
    st.session_state.started = False
    st.session_state.show_results = False
    st.session_state.saved_to_db = False
    st.session_state.start_ts = None
    st.session_state.time_limit = None
    if st.button("å‰å¾€ä¸‹ä¸€ç¯€", type="primary"): st.rerun()
    st.stop()

# ========= çµç®— =========
section_results = st.session_state.mock_section_results
section_scores = {s["section"]: int(s["score"]) for s in section_results}
total_score = int(sum(s["score"] for s in section_results))
min_each = int(min(s["score"] for s in section_results)) if section_results else 0

passed = True
fail_reason = None
if spec.get("mode") == "single":
    pass_score = int(spec.get("pass_score", 0))
    passed = total_score >= pass_score
    if not passed: fail_reason = "åˆ†æ•¸æœªé”åŠæ ¼æ¨™æº–"
else:
    pass_total = int(spec.get("pass_total", 0))
    pass_min_each = int(spec.get("pass_min_each", 0))
    passed = (total_score >= pass_total) and (min_each >= pass_min_each)
    if not passed:
        if total_score < pass_total: fail_reason = "ç¸½åˆ†ä¸è¶³"
        elif min_each < pass_min_each: fail_reason = "å–®ç§‘æœªé”æœ€ä½æ¨™æº–"

passed_db = 1 if passed else 0
all_wrong_df = pd.concat([s["wrong_df"] for s in section_results], ignore_index=True) if section_results else pd.DataFrame()
all_results_df = pd.concat([s["results_df"] for s in section_results], ignore_index=True) if section_results else pd.DataFrame()

st.session_state.mock_summary = {
    "cert_type": settings["cert_type"],
    "sections": [{"name": s["section"], "score": s["score"], "correct": s["correct"], "total": s["total"]} for s in section_results],
    "section_scores": section_scores,
    "total_score": total_score,
    "passed": passed,
    "fail_reason": fail_reason,
}
st.session_state.section_scores = section_scores
st.session_state.total_score = total_score
st.session_state.passed = passed_db
st.session_state.fail_reason = fail_reason
st.session_state.score_tuple = (int(sum(s["correct"] for s in section_results)), int(sum(s["total"] for s in section_results)), total_score)
st.session_state.wrong_df = all_wrong_df
st.session_state.results_df = all_results_df

if not st.session_state.get("saved_to_db") and st.session_state.get("mock_exam_start_ts"):
    duration_sec = int(time.time() - st.session_state.mock_exam_start_ts)
    try:
        persist_exam_record(
            user, exam_label, st.session_state.score_tuple, duration_sec, all_wrong_df,
            section_scores=section_scores, total_score=total_score, passed=passed_db, fail_reason=fail_reason
        )
        st.session_state.saved_to_db = True
    except Exception as e:
        st.error(f"å¯«å…¥æˆç¸¾å¤±æ•—ï¼š{e}")
        st.stop()

st.session_state.paper = None
st.session_state.answers = {}
st.session_state.started = False
st.session_state.show_results = False
st.session_state.start_ts = None
st.session_state.time_limit = None
st.switch_page("pages/5_æ¨¡æ“¬è€ƒ_æˆç¸¾èˆ‡éŒ¯é¡Œè§£æ.py")