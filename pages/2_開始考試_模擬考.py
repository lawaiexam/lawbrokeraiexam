import time
import pandas as pd
import streamlit as st
import random
import re

from services.state_service import ensure_state
from services.auth_service import require_login_or_render
from services.bank_service import load_bank_df
from services.exam_service import grade_paper, persist_exam_record
from services.exam_rules import CERT_CATALOG
from components.auth_ui import render_user_panel
from components.sidebar_exam_settings import render_exam_settings
from components.question_render import render_question

# ==========================================
# ğŸŸ¢ è¨­å®šå€ï¼šæ¬Šé‡èˆ‡ç« ç¯€æ˜ å°„
# ==========================================
NEW_EXAM_WEIGHTS = {
    "äººèº«ä¿éšªæ¥­å‹™å“¡è³‡æ ¼æ¸¬é©—": {
        "life_regulation": {"insurance_law_core": 40,"solicitation_rules": 40,"liability_penalties": 20},
        "life_practice": {"insurance_principles": 30,"life_products": 50,"sales_practice_ethics": 20}
    },
    "å¤–å¹£æ”¶ä»˜éæŠ•è³‡å‹ä¿éšªå•†å“æ¸¬é©—": {
        "fx_exam": {"fx_basics": 28,"fx_products": 28,"fx_regulation_compliance": 22,"fx_risk_disclosure_practice": 22}
    },
    "æŠ•è³‡å‹ä¿éšªå•†å“æ¥­å‹™å“¡æ¸¬é©—": {
        "il_regulations": {"sales_regulations": 50,"suitability_rules": 30,"dispute_liability": 20},
        "il_investment_practice": {"investment_basics": 45,"il_product_mechanics": 45,"customer_suitability_practice": 10}
    }
}

SUBJECT_IDENTIFIER = {
    "äººèº«ä¿éšªæ¥­å‹™å“¡è³‡æ ¼æ¸¬é©—": {"æ³•è¦": "life_regulation", "å¯¦å‹™": "life_practice"},
    "å¤–å¹£æ”¶ä»˜éæŠ•è³‡å‹ä¿éšªå•†å“æ¸¬é©—": {"å¤–å¹£": "fx_exam", "éæŠ•è³‡": "fx_exam"},
    "æŠ•è³‡å‹ä¿éšªå•†å“æ¥­å‹™å“¡æ¸¬é©—": {"æ³•ä»¤": "il_regulations", "è¦ç« ": "il_regulations", "ç¬¬ä¸€ç¯€": "il_regulations", "å¯¦å‹™": "il_investment_practice", "ç¬¬äºŒç¯€": "il_investment_practice"}
}

CHAPTER_MAPPING = {
    "ä¿éšªæ³•è¦": {
        "ä¿éšªå¥‘ç´„": "insurance_law_core","ä¿éšªå¥‘ç´„å…­å¤§åŸå‰‡": "insurance_law_core","å¥‘ç´„è§£é™¤ã€ç„¡æ•ˆã€å¤±æ•ˆã€åœæ•ˆã€å¾©æ•ˆ": "insurance_law_core",
        "ä¿éšªé‡‘èˆ‡è§£ç´„é‡‘": "insurance_law_core","éºç”¢ç¨…ã€è´ˆèˆ‡ç¨…": "insurance_law_core","æ‰€å¾—ç¨…": "insurance_law_core",
        "é‡‘èæ¶ˆè²»è€…ä¿è­·æ³•": "insurance_law_core","å€‹äººè³‡æ–™ä¿è­·æ³•": "insurance_law_core","æ´—éŒ¢é˜²åˆ¶æ³•": "liability_penalties", 
        "ä¿éšªæ¥­å‹™å“¡ç›¸é—œæ³•è¦åŠè¦å®š": "solicitation_rules"
    },
    "ä¿éšªå¯¦å‹™": {
        "é¢¨éšªèˆ‡é¢¨éšªç®¡ç†": "insurance_principles","äººèº«ä¿éšªæ­·å²åŠç”Ÿå‘½è¡¨": "insurance_principles","ä¿éšªè²»æ¶æ§‹ã€è§£ç´„é‡‘ã€æº–å‚™é‡‘ã€ä¿å–®ç´…åˆ©": "insurance_principles",
        "ä¿éšªä¸­é‡è¦çš„è§’è‰²": "insurance_principles","äººèº«ä¿éšªæ„ç¾©ã€åŠŸèƒ½ã€åˆ†é¡": "life_products","äººèº«ä¿éšªï¼äººå£½ä¿éšª": "life_products",
        "äººèº«ä¿éšªï¼å¹´é‡‘ä¿éšª": "life_products","äººèº«ä¿éšªï¼å¥åº·ä¿éšª": "life_products","äººèº«ä¿éšªï¼å‚·å®³ä¿éšª": "life_products",
        "äººèº«ä¿éšªï¼å…¶ä»–äººèº«ä¿éšª": "life_products","æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·": "sales_practice_ethics","ç¹¼æ‰¿ç›¸é—œ": "sales_practice_ethics"
    },
    "å¤–å¹£éæŠ•è³‡å‹": {
        "å£½éšªåŸºæœ¬æ¦‚å¿µ": "fx_basics","äººèº«ä¿éšªæ¥­è¾¦ç†ä»¥å¤–å¹£æ”¶ä»˜ä¹‹éæŠ•è³‡å‹äººèº«ä¿éšªæ¥­å‹™æ‡‰å…·å‚™è³‡æ ¼æ¢ä»¶åŠæ³¨æ„äº‹é …": "fx_products",
        "ä¿éšªæ¥­è¾¦ç†å¤–åŒ¯æ¥­å‹™ç®¡ç†è¾¦æ³•": "fx_regulation_compliance","ç®¡ç†å¤–åŒ¯æ¢ä¾‹": "fx_regulation_compliance","å¤–åŒ¯æ”¶æ”¯æˆ–äº¤æ˜“ç”³å ±è¾¦æ³•": "fx_regulation_compliance",
        "ä¿éšªæ¥­è¾¦ç†åœ‹å¤–æŠ•è³‡ç®¡ç†è¾¦æ³•": "fx_regulation_compliance","ä¿éšªæ¥­å„é¡ç›£æ§æªæ–½": "fx_regulation_compliance","éŠ·å”®æ‡‰æ³¨æ„äº‹é …": "fx_risk_disclosure_practice",
        "æ–°å‹æ…‹äººèº«ä¿éšªå•†å“å¯©æŸ¥": "fx_risk_disclosure_practice","æŠ•è³‡å‹ä¿éšªå°ˆè¨­å¸³ç°¿ä¿ç®¡æ©Ÿæ§‹åŠæŠ•è³‡æ¨™çš„æ‡‰æ³¨æ„äº‹é …": "fx_risk_disclosure_practice","æŠ•è³‡å‹ä¿éšªè§€å¿µ": "fx_products"
    },
    "æŠ•è³‡å‹æ³•è¦": {
        "æŠ•è³‡å‹ä¿éšªæ³•ä»¤ä»‹ç´¹": "sales_regulations","è­‰åˆ¸æŠ•è³‡ä¿¡è¨—åŠé¡§å•ä¹‹è¦ç¯„èˆ‡åˆ¶åº¦": "sales_regulations","éŠ·å”®æ‡‰æ³¨æ„äº‹é …": "sales_regulations",
        "é©åˆåº¦": "suitability_rules","çˆ­è­°è™•ç†": "dispute_liability"
    },
    "æŠ•è³‡å‹å¯¦å‹™": {
        "è²¨å¹£æ™‚é–“åƒ¹å€¼": "investment_basics","å‚µåˆ¸è©•åƒ¹": "investment_basics","è­‰åˆ¸è©•åƒ¹": "investment_basics","é¢¨éšªã€å ±é…¬èˆ‡æŠ•è³‡çµ„åˆ": "investment_basics",
        "è³‡æœ¬è³‡ç”¢è¨‚åƒ¹æ¨¡å¼ã€ç¸¾æ•ˆ": "investment_basics","æŠ•è³‡å·¥å…·ç°¡ä»‹": "investment_basics","é‡‘èé«”ç³»æ¦‚è¿°": "investment_basics",
        "æŠ•è³‡å‹ä¿éšªæ¦‚è«–": "il_product_mechanics","æŠ•è³‡å‹ä¿éšªè§€å¿µ": "il_product_mechanics","æŠ•è³‡å‹ä¿éšªå°ˆè¨­å¸³ç°¿ä¿ç®¡æ©Ÿæ§‹åŠæŠ•è³‡æ¨™çš„æ‡‰æ³¨æ„äº‹é …": "il_product_mechanics",
        "å®¢æˆ¶é©åˆåº¦": "customer_suitability_practice"
    }
}

# ==========================================
# ğŸŸ¢ æ ¸å¿ƒå‡½å¼
# ==========================================
def build_weighted_paper_v2(full_df, cert_type, section_name, total_questions, shuffle_options=False):
    target_col = "AIåˆ†é¡ç« ç¯€"
    if full_df.empty or target_col not in full_df.columns:
        return full_df.sample(n=min(len(full_df), total_questions)).to_dict('records')

    subject_id = None
    cert_identifiers = SUBJECT_IDENTIFIER.get(cert_type, {})
    for keyword, sid in cert_identifiers.items():
        if keyword in section_name:
            subject_id = sid
            break
            
    if not subject_id:
        return _build_paper_by_natural_distribution(full_df, total_questions)

    cert_weights = NEW_EXAM_WEIGHTS.get(cert_type, {})
    chapter_weights = cert_weights.get(subject_id, {})
    if not chapter_weights:
        return _build_paper_by_natural_distribution(full_df, total_questions)

    mapping_key_map = {
        "life_regulation": "ä¿éšªæ³•è¦", "life_practice": "ä¿éšªå¯¦å‹™",
        "fx_exam": "å¤–å¹£éæŠ•è³‡å‹",
        "il_regulations": "æŠ•è³‡å‹æ³•è¦", "il_investment_practice": "æŠ•è³‡å‹å¯¦å‹™"
    }
    mapping_category = mapping_key_map.get(subject_id)
    current_mapping = CHAPTER_MAPPING.get(mapping_category, {})

    df_temp = full_df.copy()
    df_temp["JsonChapterID"] = df_temp[target_col].map(current_mapping).fillna("others")

    exam_pool = []
    for ch_id, weight_pct in chapter_weights.items():
        target_count = int(round(total_questions * (weight_pct / 100)))
        chapter_pool = df_temp[df_temp["JsonChapterID"] == ch_id]
        take_n = min(len(chapter_pool), target_count)
        if take_n > 0:
            exam_pool.append(chapter_pool.sample(n=take_n))

    current_selected = pd.concat(exam_pool) if exam_pool else pd.DataFrame()
    needed = total_questions - len(current_selected)
    
    if needed > 0:
        others_pool = df_temp[~df_temp.index.isin(current_selected.index)]
        if not others_pool.empty:
            extra = others_pool.sample(n=min(len(others_pool), needed))
            exam_pool.append(extra)
            
    if exam_pool:
        final_df = pd.concat(exam_pool)
    else:
        final_df = pd.DataFrame()

    if len(final_df) > total_questions:
        final_df = final_df.sample(n=total_questions)

    final_df = final_df.sample(frac=1).reset_index(drop=True)
    return final_df.to_dict('records')

def _build_paper_by_natural_distribution(full_df, total_questions):
    target_col = "AIåˆ†é¡ç« ç¯€"
    if target_col not in full_df.columns:
        return full_df.sample(n=min(len(full_df), total_questions)).to_dict('records')
        
    valid_df = full_df[full_df[target_col].notna()]
    if valid_df.empty: 
        return full_df.sample(n=min(len(full_df), total_questions)).to_dict('records')

    chapter_counts = valid_df[target_col].value_counts()
    total_bank_size = len(valid_df)
    exam_pool = []
    
    for chapter, count in chapter_counts.items():
        ratio = count / total_bank_size
        n_for_chapter = int(round(total_questions * ratio))
        if n_for_chapter == 0 and count > 0: n_for_chapter = 1
        chapter_df = valid_df[valid_df[target_col] == chapter]
        exam_pool.append(chapter_df.sample(n=min(len(chapter_df), n_for_chapter)))

    exam_df = pd.concat(exam_pool) if exam_pool else pd.DataFrame()
    
    if len(exam_df) < total_questions:
        rem = valid_df[~valid_df.index.isin(exam_df.index)]
        if not rem.empty:
            exam_df = pd.concat([exam_df, rem.sample(n=min(len(rem), total_questions - len(exam_df)))])
            
    if len(exam_df) > total_questions:
        exam_df = exam_df.sample(n=total_questions)
        
    return exam_df.sample(frac=1).reset_index(drop=True).to_dict('records')

# ==========================================
# ä¸»ç¨‹å¼é–‹å§‹
# ==========================================

ensure_state()

with st.sidebar:
    render_user_panel()

user = require_login_or_render()
if user is None: st.stop()

st.title("é–‹å§‹è€ƒè©¦ - æ¨¡æ“¬è€ƒ")

with st.sidebar:
    settings = render_exam_settings(mode="mock")

spec = settings.get("mock_spec") or {}
sections = spec.get("sections") or []
if not sections:
    st.error("æ­¤è­‰ç…§é¡åˆ¥æ²’æœ‰è¨­å®šæ¨¡æ“¬è€ƒè¦å‰‡ï¼ˆMOCK_SPECSï¼‰ã€‚")
    st.stop()

if "mock_section_idx" not in st.session_state: st.session_state.mock_section_idx = 0
if "mock_section_results" not in st.session_state: st.session_state.mock_section_results = []
if "mock_exam_start_ts" not in st.session_state: st.session_state.mock_exam_start_ts = None

sec_idx = int(st.session_state.mock_section_idx)
if sec_idx >= len(sections):
    st.session_state.mock_section_idx = 0
    st.session_state.mock_section_results = []
    st.session_state.mock_exam_start_ts = None
    sec_idx = 0

section = sections[sec_idx]
section_name = section.get("name", f"Section{sec_idx+1}")
n_questions = int(section.get("n_questions", 0))
time_limit_sec = int(section.get("time_min", 0) * 60)

if n_questions <= 0:
    st.error("æ¨¡æ“¬è€ƒè¦å‰‡è¨­å®šä¸å®Œæ•´ï¼Œè«‹æª¢æŸ¥ MOCK_SPECSã€‚")
    st.stop()

try:
    bank_path = CERT_CATALOG[settings["cert_type"]]["subjects"][section_name]
except Exception:
    st.error(f"æ‰¾ä¸åˆ°é¡Œåº«æ˜ å°„ï¼š{settings['cert_type']} â†’ {section_name}")
    st.stop()

df = load_bank_df(settings.get("cert_type", ""), merge_all=False, bank_source_path=bank_path)

if df is None or df.empty:
    st.warning("å°šæœªè¼‰å…¥é¡Œåº«ï¼Œè«‹ç¢ºèªé¡Œåº«æª”æ¡ˆæ˜¯å¦å­˜åœ¨ã€‚")
    st.stop()

# ==========================================
# ğŸš‘ HOTFIX V4: çµ‚æ¥µå…¨èƒ½è³‡æ–™æ¸…æ´—è£œä¸ (The Universal Cleaner)
# ==========================================
try:
    df.columns = df.columns.str.strip()

    # 1. çµ±ä¸€ ID
    if "ID" not in df.columns:
        if "ç·¨è™Ÿ" in df.columns: df["ID"] = df["ç·¨è™Ÿ"]
        elif "é¡Œç›®ç·¨è™Ÿ" in df.columns: df["ID"] = df["é¡Œç›®ç·¨è™Ÿ"]
        else: df["ID"] = range(1, len(df) + 1)

    # å®šç¾©æ˜ å°„: æ”¯æ´ä¸­æ–‡(äººèº«)èˆ‡æ•¸å­—(æŠ•è³‡å‹)
    option_map_config = [
        ('A', ['é¸é …ä¸€', 'é¸é …1', 'Option A', 'A']),
        ('B', ['é¸é …äºŒ', 'é¸é …2', 'Option B', 'B']),
        ('C', ['é¸é …ä¸‰', 'é¸é …3', 'Option C', 'C']),
        ('D', ['é¸é …å››', 'é¸é …4', 'Option D', 'D']),
        ('E', ['é¸é …äº”', 'é¸é …5', 'Option E', 'E'])
    ]

    # 2. è™•ç† Answer
    if "Answer" not in df.columns:
        if "æ­£ç¢ºé¸é …" in df.columns:
            def normalize_answer(val):
                val_str = str(val).strip()
                mapping = {'1': 'A', '2': 'B', '3': 'C', '4': 'D', '5': 'E'}
                return mapping.get(val_str, val_str)
            df["Answer"] = df["æ­£ç¢ºé¸é …"].apply(normalize_answer)
        else:
            def extract_star_answer(row):
                for label, possible_cols in option_map_config:
                    for col in possible_cols:
                        if col in row and pd.notna(row[col]):
                            if str(row[col]).strip().startswith("*"):
                                return label
                return ""
            df["Answer"] = df.apply(extract_star_answer, axis=1)

            # æ´—æ‰æ˜Ÿè™Ÿ
            all_opt_cols = [col for _, cols in option_map_config for col in cols]
            for c in all_opt_cols:
                if c in df.columns:
                    df[c] = df[c].apply(lambda x: str(x).lstrip('*') if pd.notna(x) else x)

    # 3. æ‰“åŒ… Choices
    if "Choices" not in df.columns:
        def universal_pack(row):
            choices = []
            for label, possible_cols in option_map_config:
                found_text = None
                for col in possible_cols:
                    if col in row and pd.notna(row[col]):
                        val = str(row[col]).strip()
                        if val and val.lower() != "nan":
                            found_text = val
                            break
                if found_text: choices.append((label, found_text))
            return choices
        df["Choices"] = df.apply(universal_pack, axis=1)

except Exception as e:
    st.error(f"è³‡æ–™æ ¼å¼è½‰æ›å¤±æ•—ï¼š{e}")
    st.stop()
# ==========================================
# ğŸš‘ è£œä¸çµæŸ
# ==========================================

st.session_state.df = df
filtered = df
exam_label = f"{settings['cert_type']}ï½œæ¨¡æ“¬è€ƒ"
st.session_state.current_bank_name = exam_label

with st.expander("æœ¬æ¬¡æ¨¡æ“¬è€ƒè¦æ ¼", expanded=True):
    st.write(f"- é¡åˆ¥ï¼š{settings['cert_type']}")
    st.write(f"- æ¨¡å¼ï¼š{'å…©ç¯€é€£è€ƒ' if len(sections) > 1 else 'å–®ç¯€'}")
    
    subject_id = None
    for kw, sid in SUBJECT_IDENTIFIER.get(settings["cert_type"], {}).items():
        if kw in section_name:
            subject_id = sid
            break
            
    if subject_id:
        weights = NEW_EXAM_WEIGHTS[settings["cert_type"]].get(subject_id, {})
        st.info(f"ğŸ’¡ æœ¬ç¯€ ({section_name}) æ¡ç”¨æ¬Šé‡æŠ½æ¨£ï¼š\n" + ", ".join([f"{k}:{v}%" for k,v in weights.items()]))
    else:
        st.write("ğŸ’¡ æœ¬ç¯€æ¡ç”¨è‡ªç„¶åˆ†ä½ˆæŠ½æ¨£")

    st.write("")
    for i, s in enumerate(sections, start=1):
        st.write(f"- ç¬¬ {i} ç¯€ï¼š{s['name']}ï½œ{s['n_questions']} é¡Œï½œ{s['time_min']} åˆ†é˜")

st.divider()
st.subheader(f"ç¬¬ {sec_idx+1} ç¯€ï¼š{section_name}")

colA, colB = st.columns([1, 1])

def _reset_whole_mock_exam():
    for k in ["paper", "answers", "started", "show_results", "saved_to_db", "start_ts", "time_limit"]:
        if k in st.session_state: del st.session_state[k]
    st.session_state.mock_section_idx = 0
    st.session_state.mock_section_results = []
    st.session_state.mock_exam_start_ts = None
    for k in ["mock_summary", "score_tuple", "wrong_df", "results_df", "section_scores", "total_score", "passed", "fail_reason"]:
        if k in st.session_state: del st.session_state[k]

with colA:
    if st.button("é–‹å§‹æœ¬ç¯€", type="primary"):
        st.session_state.paper = build_weighted_paper_v2(
            filtered,
            settings["cert_type"],
            section_name,
            n_questions,
            shuffle_options=settings["shuffle_options"]
        )
        st.session_state.answers = {}
        st.session_state.started = True
        st.session_state.show_results = False
        st.session_state.saved_to_db = False
        st.session_state.start_ts = time.time()
        if st.session_state.mock_exam_start_ts is None:
            st.session_state.mock_exam_start_ts = st.session_state.start_ts
        st.session_state.time_limit = time_limit_sec
        st.rerun()

with colB:
    if st.button("é‡ç½®æ•´å ´æ¨¡æ“¬è€ƒ", type="secondary"):
        _reset_whole_mock_exam()
        st.rerun()

paper = st.session_state.get("paper")
if not paper:
    st.info("è«‹å…ˆæŒ‰ã€Œé–‹å§‹æœ¬ç¯€ã€ã€‚")
    st.stop()

if st.session_state.get("time_limit") and st.session_state.get("start_ts"):
    elapsed = int(time.time() - st.session_state.start_ts)
    remain = max(0, st.session_state.time_limit - elapsed)
    mins, secs = divmod(remain, 60)
    st.metric("æœ¬ç¯€å‰©é¤˜æ™‚é–“", f"{mins} åˆ† {secs:02d} ç§’")

    if remain == 0 and not st.session_state.get("show_results"):
        st.warning("æ™‚é–“åˆ°ï¼Œè‡ªå‹•äº¤å·ã€‚")
        st.session_state.show_results = True
        st.rerun()

if not st.session_state.get("show_results"):
    st.subheader("ä½œç­”å€")
    for idx, q in enumerate(paper, start=1):
        with st.expander(f"ç¬¬ {idx} é¡Œ", expanded=(idx == 1)):
            picked = render_question(q, show_image=settings["show_image"], answer_key=f"mock_s{sec_idx}_ans_{q['ID']}")
            st.session_state.answers[q["ID"]] = picked

    if st.button("äº¤å·ï¼ˆæœ¬ç¯€ï¼‰", type="primary"):
        st.session_state.show_results = True
        st.rerun()

if not st.session_state.get("show_results"): st.stop()

results_df, score_tuple, wrong_df = grade_paper(paper, st.session_state.answers)
correct, total, score = score_tuple

st.session_state.mock_section_results.append({
    "section": section_name,
    "score": int(score),
    "correct": int(correct),
    "total": int(total),
    "results_df": results_df,
    "wrong_df": wrong_df,
})

st.session_state.mock_section_idx += 1

if st.session_state.mock_section_idx < len(sections):
    st.success(f"å·²å®Œæˆç¬¬ {sec_idx+1} ç¯€ï¼š{section_name}ï¼ˆ{score} åˆ†ï¼‰ã€‚")
    st.session_state.paper = None
    st.session_state.answers = {}
    st.session_state.started = False
    st.session_state.show_results = False
    st.session_state.saved_to_db = False
    st.session_state.start_ts = None
    st.session_state.time_limit = None
    if st.button("å‰å¾€ä¸‹ä¸€ç¯€", type="primary"): st.rerun()
    st.stop()

section_results = st.session_state.mock_section_results
section_scores = {s["section"]: int(s["score"]) for s in section_results}
total_score = int(sum(s["score"] for s in section_results))
min_each = int(min(s["score"] for s in section_results)) if section_results else 0

passed = True
fail_reason = None
if spec.get("mode") == "single":
    pass_score = int(spec.get("pass_score", 0))
    passed = total_score >= pass_score
    if not passed: fail_reason = "åˆ†æ•¸æœªé”åŠæ ¼æ¨™æº–"
else:
    pass_total = int(spec.get("pass_total", 0))
    pass_min_each = int(spec.get("pass_min_each", 0))
    passed = (total_score >= pass_total) and (min_each >= pass_min_each)
    if not passed:
        if total_score < pass_total: fail_reason = "ç¸½åˆ†ä¸è¶³"
        elif min_each < pass_min_each: fail_reason = "å–®ç§‘æœªé”æœ€ä½æ¨™æº–"

passed_db = 1 if passed else 0
all_wrong_df = pd.concat([s["wrong_df"] for s in section_results], ignore_index=True) if section_results else pd.DataFrame()
all_results_df = pd.concat([s["results_df"] for s in section_results], ignore_index=True) if section_results else pd.DataFrame()

st.session_state.mock_summary = {
    "cert_type": settings["cert_type"],
    "sections": [{"name": s["section"], "score": s["score"], "correct": s["correct"], "total": s["total"]} for s in section_results],
    "section_scores": section_scores,
    "total_score": total_score,
    "passed": passed,
    "fail_reason": fail_reason,
}
st.session_state.section_scores = section_scores
st.session_state.total_score = total_score
st.session_state.passed = passed_db
st.session_state.fail_reason = fail_reason
st.session_state.score_tuple = (int(sum(s["correct"] for s in section_results)), int(sum(s["total"] for s in section_results)), total_score)
st.session_state.wrong_df = all_wrong_df
st.session_state.results_df = all_results_df

if not st.session_state.get("saved_to_db") and st.session_state.get("mock_exam_start_ts"):
    duration_sec = int(time.time() - st.session_state.mock_exam_start_ts)
    try:
        persist_exam_record(
            user, exam_label, st.session_state.score_tuple, duration_sec, all_wrong_df,
            section_scores=section_scores, total_score=total_score, passed=passed_db, fail_reason=fail_reason
        )
        st.session_state.saved_to_db = True
    except Exception as e:
        st.error(f"å¯«å…¥æˆç¸¾å¤±æ•—ï¼š{e}")
        st.stop()

st.session_state.paper = None
st.session_state.answers = {}
st.session_state.started = False
st.session_state.show_results = False
st.session_state.start_ts = None
st.session_state.time_limit = None
st.switch_page("pages/5_æ¨¡æ“¬è€ƒ_æˆç¸¾èˆ‡éŒ¯é¡Œè§£æ.py")
