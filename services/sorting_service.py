import pandas as pd
import os
import re
import time
import difflib  # ç”¨æ–¼æ¨¡ç³Šæ¯”å°
from io import BytesIO
from collections import Counter
from google import genai
from google.genai import types
import streamlit as st
from utils import github_handler as gh  # å¼•å…¥ GitHub å·¥å…·

# ==========================================
# è¨­å®šå€
# ==========================================

# ç­†è¨˜è·¯å¾‘ï¼šå‡è¨­ç­†è¨˜æ”¾åœ¨ bank/{folder}/ ä¸‹
BASE_BANK_DIR = "bank"

# API Key
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "AIzaSyCiNkDK8pfn305ZSlHmWbVj89_sXBl2eqo")

EXAM_CONFIGS = {
    "äººèº«ä¿éšª": {
        "folder": "äººèº«",
        "note_file": "ç­†è¨˜_äººèº«.pdf",
        "col_opts": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"], 
        "outputs": [
            {
                "filename": "äººèº«_ä¿éšªæ³•è¦.xlsx",
                "chapters": [
                    "ä¿éšªä¸­é‡è¦çš„è§’è‰²", "ä¿éšªå¥‘ç´„", "ä¿éšªå¥‘ç´„å…­å¤§åŸå‰‡", "å¥‘ç´„è§£é™¤ã€ç„¡æ•ˆã€å¤±æ•ˆã€åœæ•ˆã€å¾©æ•ˆ",
                    "ä¿éšªé‡‘èˆ‡è§£ç´„é‡‘", "ç¹¼æ‰¿ç›¸é—œ", "éºç”¢ç¨…ã€è´ˆèˆ‡ç¨…", "æ‰€å¾—ç¨…",
                    "ä¿éšªæ¥­å‹™å“¡ç›¸é—œæ³•è¦åŠè¦å®š", "é‡‘èæ¶ˆè²»è€…ä¿è­·æ³•", "å€‹äººè³‡æ–™ä¿è­·æ³•", "æ´—éŒ¢é˜²åˆ¶æ³•"
                ]
            },
            {
                "filename": "äººèº«_ä¿éšªå¯¦å‹™.xlsx",
                "chapters": [
                    "é¢¨éšªèˆ‡é¢¨éšªç®¡ç†", "äººèº«ä¿éšªæ­·å²åŠç”Ÿå‘½è¡¨", "ä¿éšªè²»æ¶æ§‹ã€è§£ç´„é‡‘ã€æº–å‚™é‡‘ã€ä¿å–®ç´…åˆ©",
                    "äººèº«ä¿éšªæ„ç¾©ã€åŠŸèƒ½ã€åˆ†é¡", "äººèº«ä¿éšªï¼äººå£½ä¿éšª", "äººèº«ä¿éšªï¼å¹´é‡‘ä¿éšª",
                    "äººèº«ä¿éšªï¼å¥åº·ä¿éšª", "äººèº«ä¿éšªï¼å‚·å®³ä¿éšª", "äººèº«ä¿éšªï¼å…¶ä»–äººèº«ä¿éšª", "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·"
                ]
            }
        ],
        "default_chapter": "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·"
    },
    "æŠ•è³‡å‹ä¿éšª": {
        "folder": "æŠ•è³‡å‹",
        "note_file": "ç­†è¨˜_æŠ•è³‡å‹.pdf",
        "col_opts": ["é¸é …1", "é¸é …2", "é¸é …3", "é¸é …4"], 
        "outputs": [
            {
                "filename": "æŠ•è³‡å‹_æ³•ä»¤è¦ç« .xlsx",
                "chapters": [
                    "æŠ•è³‡å‹ä¿éšªæ¦‚è«–", "æŠ•è³‡å‹ä¿éšªæ³•ä»¤ä»‹ç´¹", "é‡‘èé«”ç³»æ¦‚è¿°", "è­‰åˆ¸æŠ•è³‡ä¿¡è¨—åŠé¡§å•ä¹‹è¦ç¯„èˆ‡åˆ¶åº¦"
                ]
            },
            {
                "filename": "æŠ•è³‡å‹_æŠ•è³‡å¯¦å‹™.xlsx",
                "chapters": [
                    "è²¨å¹£æ™‚é–“åƒ¹å€¼", "å‚µåˆ¸è©•åƒ¹", "è­‰åˆ¸è©•åƒ¹", "é¢¨éšªã€å ±é…¬èˆ‡æŠ•è³‡çµ„åˆ",
                    "è³‡æœ¬è³‡ç”¢è¨‚åƒ¹æ¨¡å¼ã€ç¸¾æ•ˆ", "æŠ•è³‡å·¥å…·ç°¡ä»‹"
                ]
            }
        ],
        "default_chapter": "æŠ•è³‡å‹ä¿éšªæ¦‚è«–"
    },
    "å¤–å¹£ä¿å–®": {
        "folder": "å¤–å¹£",
        "note_file": "ç­†è¨˜_å¤–å¹£.pdf",
        "col_opts": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"],
        "outputs": [
            {
                "filename": "å¤–å¹£.xlsx", 
                "chapters": [
                    "å£½éšªåŸºæœ¬æ¦‚å¿µ", "ä¿éšªæ¥­è¾¦ç†å¤–åŒ¯æ¥­å‹™ç®¡ç†è¾¦æ³•", "ç®¡ç†å¤–åŒ¯æ¢ä¾‹", "å¤–åŒ¯æ”¶æ”¯æˆ–äº¤æ˜“ç”³å ±è¾¦æ³•",
                    "ä¿éšªæ¥­è¾¦ç†åœ‹å¤–æŠ•è³‡ç®¡ç†è¾¦æ³•", "äººèº«ä¿éšªæ¥­è¾¦ç†ä»¥å¤–å¹£æ”¶ä»˜ä¹‹éæŠ•è³‡å‹äººèº«ä¿éšªæ¥­å‹™æ‡‰å…·å‚™è³‡æ ¼æ¢ä»¶åŠæ³¨æ„äº‹é …",
                    "æŠ•è³‡å‹ä¿éšªè§€å¿µ", "æŠ•è³‡å‹ä¿éšªå°ˆè¨­å¸³ç°¿ä¿ç®¡æ©Ÿæ§‹åŠæŠ•è³‡æ¨™çš„æ‡‰æ³¨æ„äº‹é …",
                    "éŠ·å”®æ‡‰æ³¨æ„äº‹é …", "æ–°å‹æ…‹äººèº«ä¿éšªå•†å“å¯©æŸ¥", "ä¿éšªæ¥­å„é¡ç›£æ§æªæ–½"
                ]
            }
        ],
        "default_chapter": "å£½éšªåŸºæœ¬æ¦‚å¿µ"
    }
}

COL_Q = "é¡Œç›®"

# ==========================================
# å·¥å…·é¡åˆ¥ (Client & Logic)
# ==========================================

class GeminiClient:
    def __init__(self, api_key):
        self.client = genai.Client(api_key=api_key)
        self.model_name = "gemini-2.5-flash"
        
    def generate(self, prompt, temperature=0.1):
        for attempt in range(3):
            try:
                response = self.client.models.generate_content(
                    model=self.model_name, contents=prompt,
                    config=types.GenerateContentConfig(temperature=temperature)
                )
                return response.text.strip()
            except Exception:
                time.sleep((attempt + 1) * 2)
        return ""

import pdfplumber

class ChapterManager:
    def __init__(self, folder_name, pdf_filename, all_chapters, ai_client):
        # é›²ç«¯ç‰ˆï¼šå˜—è©¦å¾ GitHub ä¸‹è¼‰ç­†è¨˜å…§å®¹
        self.pdf_path = f"{BASE_BANK_DIR}/{folder_name}/{pdf_filename}"
        self.all_chapters = all_chapters
        self.ai = ai_client
        self.full_note_text = ""
        self.chapter_keywords = {} 
        self._read_pdf_from_github()
        self._gen_keywords()

    def _read_pdf_from_github(self):
        # ä½¿ç”¨ gh_download_bytes è®€å–ç­†è¨˜
        data = gh.gh_download_bytes(self.pdf_path)
        if not data:
            return
        
        try:
            with pdfplumber.open(BytesIO(data)) as pdf:
                content = []
                for page in pdf.pages:
                    txt = page.extract_text()
                    if txt: content.append(txt)
                self.full_note_text = "\n".join(content)
        except Exception:
            pass

    def _get_context(self, chapter):
        if not self.full_note_text: return ""
        terms = re.split(r'[ã€\s\-\(\)è¾¦æ³•æ³¨æ„äº‹é …]', chapter)
        terms = [t for t in terms if len(t) >= 2]
        terms.append(chapter) 
        snippets = []
        for t in terms:
            indices = [m.start() for m in re.finditer(re.escape(t), self.full_note_text)]
            for idx in indices[:2]:
                snippets.append(self.full_note_text[max(0, idx-200):min(len(self.full_note_text), idx+500)])
        return "\n...\n".join(snippets)

    def _gen_keywords(self):
        if not self.full_note_text:
            for ch in self.all_chapters:
                self.chapter_keywords[ch] = [ch]
            return

        progress_text = "æ­£åœ¨åˆ†æç­†è¨˜èˆ‡å»ºç«‹é—œéµå­—..."
        my_bar = st.progress(0, text=progress_text)
        
        total = len(self.all_chapters)
        for i, ch in enumerate(self.all_chapters):
            ctx = self._get_context(ch)
            prompt = (
                f"ä½ æ˜¯ä¿éšªè€ƒé¡Œåˆ†é¡å°ˆå®¶ã€‚è«‹é‡å°ç« ç¯€ã€{ch}ã€ï¼Œåˆ—å‡º 5-10 å€‹æ ¸å¿ƒã€Œå°ˆæœ‰åè©ã€æˆ–ã€Œé—œéµå­—ã€ã€‚\n"
                f"åƒè€ƒç­†è¨˜ï¼š\n{ctx}\nåªè¼¸å‡ºé—œéµå­—ï¼Œç”¨é€—è™Ÿåˆ†éš”ã€‚"
            )
            res = self.ai.generate(prompt)
            if res:
                kws = [k.strip() for k in res.replace("ã€", ",").replace("\n", ",").split(",") if len(k.strip())>1]
                self.chapter_keywords[ch] = kws[:15]
            else:
                self.chapter_keywords[ch] = [ch]
            
            my_bar.progress((i + 1) / total, text=f"åˆ†æç« ç¯€ï¼š{ch}")
            time.sleep(0.2)
        
        my_bar.empty()

class SmartClassifier:
    def __init__(self, mgr, default_ch):
        self.mgr = mgr
        self.default_ch = default_ch
        # å»ºç«‹ä¸€å€‹ä¹¾æ·¨çš„ç« ç¯€æ¸…å–®å­—ä¸²ï¼Œè®“ AI å¥½è®€
        self.chapters_str = "\n".join([f"- {c}" for c in mgr.all_chapters])

    def classify(self, q, opts):
        full_text = f"{q} {opts}"
        
        # 1. é—œéµå­—è¦å‰‡æ¯”å° (Rule-Based) - å…ˆæ¶å¿«
        scores = Counter()
        for ch, kws in self.mgr.chapter_keywords.items():
            for kw in kws:
                if kw in full_text: 
                    # è‹¥é—œéµå­—è·Ÿç« ç¯€åå®Œå…¨ä¸€æ¨£ï¼Œæ¬Šé‡åŠ é‡
                    weight = 5 if kw == ch else 1
                    scores[ch] += weight
        
        if scores:
            best_chapter, score = scores.most_common(1)[0]
            # é–€æª»å€¼ï¼šè‡³å°‘è¦æœ‰ 2 åˆ†æ‰ç®—æ•¸
            if score >= 2: 
                return best_chapter, "é—œéµå­—"

        # 2. AI èªæ„åˆ¤æ–· (AI-Based) - è™•ç†é›£é¡Œ
        prompt = (
            f"ä½ æ˜¯ä¸€å€‹ä¿éšªè€ƒé¡Œåˆ†é¡å“¡ã€‚è«‹æ ¹æ“šé¡Œç›®èˆ‡é¸é …ï¼Œå¾ä¸‹æ–¹ã€æ¨™æº–ç« ç¯€æ¸…å–®ã€ä¸­ï¼Œé¸å‡ºæœ€ç›¸é—œçš„ä¸€å€‹ç« ç¯€ã€‚\n"
            f"é¡Œç›®ï¼š{q}\n"
            f"é¸é …ï¼š{opts}\n\n"
            f"ã€æ¨™æº–ç« ç¯€æ¸…å–®ã€‘ï¼š\n{self.chapters_str}\n\n"
            f"æ³¨æ„ï¼šä½ åªèƒ½è¼¸å‡ºæ¸…å–®ä¸­çš„åç¨±ï¼Œä¸è¦è¼¸å‡ºä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡‹ã€‚"
        )
        
        ai_response = self.mgr.ai.generate(prompt)
        ai_response = ai_response.strip().replace("\n", "").replace(" ", "") # æ¸…ç† AI å›ç­”

        # 3. æ¨¡ç³Šæ¯”å° (Fuzzy Match) - è§£æ±º "AI å¤šå˜´" çš„å•é¡Œ
        # æ–¹æ³• A: ç›´æ¥åŒ…å«æª¢æŸ¥
        for ch in self.mgr.all_chapters:
            if ch in ai_response: 
                return ch, "AIåˆ¤æ–·"
        
        # æ–¹æ³• B: ç›¸ä¼¼åº¦æ¯”å° (Similarity) - é‡å° AI å¯«éŒ¯å­—æˆ–å¤šå­—çš„ç‹€æ³
        # æ‰¾å‡ºèˆ‡ AI å›ç­”æœ€åƒçš„ç« ç¯€
        matches = difflib.get_close_matches(ai_response, self.mgr.all_chapters, n=1, cutoff=0.4)
        if matches:
            return matches[0], "AI(æ¨¡ç³Š)"

        # 4. çœŸçš„æ²’æ•‘äº†ï¼Œå›å‚³é è¨­å€¼
        return self.default_ch, "é è¨­"

# ==========================================
# å°å¤–ä»‹é¢å‡½æ•¸
# ==========================================

# ğŸ‘‡ã€å¿«å–æ ¸å¿ƒã€‘é€™å€‹å‡½å¼è² è²¬å»ºç«‹ä¸¦ã€Œè¨˜ä½ã€Managerï¼Œé¿å…æ¯æ¬¡é‡è·‘
@st.cache_resource(show_spinner=False)
def get_cached_manager(folder_name, note_filename, all_chapters_tuple):
    # é€™è£¡å¿…é ˆæŠŠ list è½‰æˆ tuple æ‰èƒ½è¢« cacheï¼Œè£¡é¢å†è½‰å› list
    client = GeminiClient(GEMINI_API_KEY)
    return ChapterManager(folder_name, note_filename, list(all_chapters_tuple), client)

def process_uploaded_file(exam_type, uploaded_file):
    config = EXAM_CONFIGS.get(exam_type)
    if not config: return None

    all_chapters = []
    for output_conf in config['outputs']:
        all_chapters.extend(output_conf['chapters'])

    # ğŸ‘‡ã€ä¿®æ”¹ã€‘ä½¿ç”¨å¿«å–æ©Ÿåˆ¶å–å¾— Manager
    # æ³¨æ„ï¼šæˆ‘å€‘æŠŠ all_chapters (list) è½‰æˆ tuple å‚³é€²å»ï¼Œå› ç‚º list ä¸èƒ½è¢« Streamlit cache hash
    mgr = get_cached_manager(config['folder'], config['note_file'], tuple(all_chapters))
    
    classifier = SmartClassifier(mgr, config['default_chapter'])

    try:
        dfs = pd.read_excel(uploaded_file, sheet_name=None)
    except Exception as e:
        st.error(f"Excel è®€å–å¤±æ•—: {e}")
        return None

    results = []
    curr_sheet = 0
    progress_bar = st.progress(0, text="é–‹å§‹åˆ†é¡é¡Œç›®...")

    for name, df in dfs.items():
        curr_sheet += 1
        if df.empty or COL_Q not in df.columns: 
            continue
        
        valid_opts = [c for c in config['col_opts'] if c in df.columns]
        total_rows = len(df)
        
        for idx, row in df.iterrows():
            q = str(row.get(COL_Q, "")).strip()
            if not q or q.lower() == "nan": continue
            
            opts_txt = " ".join([str(row.get(c, "")) for c in valid_opts])
            ch, src = classifier.classify(q, opts_txt)
            
            r = row.to_dict()
            r["AIåˆ†é¡ç« ç¯€"] = ch
            r["åˆ†é¡ä¾†æº"] = src
            results.append(r)
            
            # æ¯ 5 é¡Œæ›´æ–°ä¸€æ¬¡é€²åº¦ï¼Œé¿å… UI å¡é “
            if idx % 5 == 0:
                progress = (idx + 1) / total_rows
                progress_bar.progress(progress, text=f"æ­£åœ¨è™•ç†åˆ†é  '{name}'ï¼šç¬¬ {idx+1}/{total_rows} é¡Œ")

    progress_bar.empty()
    return pd.DataFrame(results)

def save_merged_results(exam_type, new_classified_df):
    """
    å°‡åˆ†é¡å¥½çš„ DF åˆä½µå› GitHub ä¸Šçš„é¡Œåº«
    """
    config = EXAM_CONFIGS.get(exam_type)
    # GitHub ä¸Šçš„è³‡æ–™å¤¾è·¯å¾‘ (ä¾‹å¦‚ bank/äººèº«)
    base_gh_path = f"{BASE_BANK_DIR}/{config['folder']}"
    
    logs = []

    for out_conf in config['outputs']:
        filename = out_conf['filename']
        target_chs = out_conf['chapters']
        # GitHub å®Œæ•´æª”æ¡ˆè·¯å¾‘
        target_gh_path = f"{base_gh_path}/{filename}"

        # ç¯©é¸æ–°é¡Œç›®
        sub_new = new_classified_df[new_classified_df["AIåˆ†é¡ç« ç¯€"].isin(target_chs)].copy()
        if sub_new.empty:
            continue

        # 1. å˜—è©¦å¾ GitHub ä¸‹è¼‰èˆŠæª”
        existing_df = pd.DataFrame()
        old_file_bytes = gh.gh_download_bytes(target_gh_path)
        
        if old_file_bytes:
            try:
                xls = pd.read_excel(BytesIO(old_file_bytes), sheet_name=None)
                for sname, sdf in xls.items():
                    if "AIåˆ†é¡ç« ç¯€" not in sdf.columns:
                        sdf["AIåˆ†é¡ç« ç¯€"] = sname
                    existing_df = pd.concat([existing_df, sdf], ignore_index=True)
            except Exception:
                pass

        # 2. åˆä½µèˆ‡å»é‡
        if not existing_df.empty:
            common = list(set(existing_df.columns) & set(sub_new.columns))
            if COL_Q in common:
                combined = pd.concat([existing_df, sub_new], ignore_index=True)
            else:
                combined = sub_new
        else:
            combined = sub_new
        
        before = len(combined)
        combined.drop_duplicates(subset=[COL_Q], keep='last', inplace=True)
        after = len(combined)
        removed = before - after
        
        logs.append(f"ğŸ“„ **{filename}**ï¼šæ–°å¢ {len(sub_new)} é¡Œï¼Œåˆä½µå¾Œå…± {after} é¡Œ (å·²è‡ªå‹•ç§»é™¤ {removed} é¡Œé‡è¤‡)ã€‚")

        # 3. è½‰å­˜ç‚º Excel Bytes
        mapper = {name: i for i, name in enumerate(target_chs)}
        combined["Sort"] = combined["AIåˆ†é¡ç« ç¯€"].map(mapper).fillna(999)
        combined = combined.sort_values("Sort")

        output = BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            for ch in target_chs:
                ch_df = combined[combined["AIåˆ†é¡ç« ç¯€"] == ch]
                if not ch_df.empty:
                    safe = ch.replace("/", "_")[:30]
                    ch_df.drop(columns=["Sort"], errors="ignore").to_excel(writer, sheet_name=safe, index=False)
        
        # 4. ä¸Šå‚³å› GitHub (è¦†è“‹èˆŠæª”)
        file_bytes = output.getvalue()
        gh.gh_put_file(
            target_gh_path, 
            file_bytes, 
            f"Auto-Merge: Updated {filename} via Admin Panel"
        )

    return logs