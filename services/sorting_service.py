import pandas as pd
import os
import re
import time
import json
import difflib
import pdfplumber
from io import BytesIO
from collections import Counter
from google import genai
from google.genai import types
import streamlit as st
from utils import github_handler as gh

# ==========================================
# è¨­å®šå€ (EXAM_CONFIGS) - å·²æ ¹æ“šæ‚¨æä¾›çš„ pa, ipa, fci èˆŠæª”æ¡ˆé€²è¡Œæ ¡æ­£
# ==========================================
BASE_BANK_DIR = "bank"
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")

EXAM_CONFIGS = {
    # ä¾†è‡ª pa_sorting.py çš„é‚è¼¯
    "äººèº«ä¿éšª": {
        "folder": "äººèº«",
        "note_file": "ç­†è¨˜_äººèº«.pdf",
        "col_opts": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"], 
        "outputs": [
            {
                "filename": "äººèº«_ä¿éšªæ³•è¦.xlsx",
                "chapters": [
                    "ä¿éšªä¸­é‡è¦çš„è§’è‰²", "ä¿éšªå¥‘ç´„", "ä¿éšªå¥‘ç´„å…­å¤§åŸå‰‡", "å¥‘ç´„è§£é™¤ã€ç„¡æ•ˆã€å¤±æ•ˆã€åœæ•ˆã€å¾©æ•ˆ",
                    "ä¿éšªé‡‘èˆ‡è§£ç´„é‡‘", "ç¹¼æ‰¿ç›¸é—œ", "éºç”¢ç¨…ã€è´ˆèˆ‡ç¨…", "æ‰€å¾—ç¨…",
                    "ä¿éšªæ¥­å‹™å“¡ç›¸é—œæ³•è¦åŠè¦å®š", "é‡‘èæ¶ˆè²»è€…ä¿è­·æ³•", "å€‹äººè³‡æ–™ä¿è­·æ³•", "æ´—éŒ¢é˜²åˆ¶æ³•"
                ]
            },
            {
                "filename": "äººèº«_ä¿éšªå¯¦å‹™.xlsx",
                "chapters": [
                    "é¢¨éšªèˆ‡é¢¨éšªç®¡ç†", "äººèº«ä¿éšªæ­·å²åŠç”Ÿå‘½è¡¨", "ä¿éšªè²»æ¶æ§‹ã€è§£ç´„é‡‘ã€æº–å‚™é‡‘ã€ä¿å–®ç´…åˆ©",
                    "äººèº«ä¿éšªæ„ç¾©ã€åŠŸèƒ½ã€åˆ†é¡", "äººèº«ä¿éšªï¼äººå£½ä¿éšª", "äººèº«ä¿éšªï¼å¹´é‡‘ä¿éšª",
                    "äººèº«ä¿éšªï¼å¥åº·ä¿éšª", "äººèº«ä¿éšªï¼å‚·å®³ä¿éšª", "äººèº«ä¿éšªï¼å…¶ä»–äººèº«ä¿éšª", "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·"
                ]
            }
        ],
        "default_chapter": "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·"
    },
    
    # ä¾†è‡ª ipa_sorting.py çš„é‚è¼¯
    "æŠ•è³‡å‹ä¿éšª": {
        "folder": "æŠ•è³‡å‹",
        "note_file": "ç­†è¨˜_æŠ•è³‡å‹.pdf",
        "col_opts": ["é¸é …1", "é¸é …2", "é¸é …3", "é¸é …4"], 
        "outputs": [
            {
                "filename": "æŠ•è³‡å‹_æ³•ä»¤è¦ç« .xlsx",
                "chapters": [
                    "æŠ•è³‡å‹ä¿éšªæ¦‚è«–", "æŠ•è³‡å‹ä¿éšªæ³•ä»¤ä»‹ç´¹", "é‡‘èé«”ç³»æ¦‚è¿°", "è­‰åˆ¸æŠ•è³‡ä¿¡è¨—åŠé¡§å•ä¹‹è¦ç¯„èˆ‡åˆ¶åº¦"
                ]
            },
            {
                "filename": "æŠ•è³‡å‹_æŠ•è³‡å¯¦å‹™.xlsx",
                "chapters": [
                    "è²¨å¹£æ™‚é–“åƒ¹å€¼", "å‚µåˆ¸è©•åƒ¹", "è­‰åˆ¸è©•åƒ¹", "é¢¨éšªã€å ±é…¬èˆ‡æŠ•è³‡çµ„åˆ",
                    "è³‡æœ¬è³‡ç”¢è¨‚åƒ¹æ¨¡å¼ã€ç¸¾æ•ˆ", "æŠ•è³‡å·¥å…·ç°¡ä»‹"
                ]
            }
        ],
        "default_chapter": "æŠ•è³‡å‹ä¿éšªæ¦‚è«–"
    },
    
    # ä¾†è‡ª fci_sorting.py çš„é‚è¼¯
    "å¤–å¹£ä¿å–®": {
        "folder": "å¤–å¹£",
        "note_file": "ç­†è¨˜_å¤–å¹£.pdf",
        "col_opts": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"],
        "outputs": [
            {
                "filename": "å¤–å¹£.xlsx", 
                "chapters": [
                    "å£½éšªåŸºæœ¬æ¦‚å¿µ", "ä¿éšªæ¥­è¾¦ç†å¤–åŒ¯æ¥­å‹™ç®¡ç†è¾¦æ³•", "ç®¡ç†å¤–åŒ¯æ¢ä¾‹", "å¤–åŒ¯æ”¶æ”¯æˆ–äº¤æ˜“ç”³å ±è¾¦æ³•",
                    "ä¿éšªæ¥­è¾¦ç†åœ‹å¤–æŠ•è³‡ç®¡ç†è¾¦æ³•", "äººèº«ä¿éšªæ¥­è¾¦ç†ä»¥å¤–å¹£æ”¶ä»˜ä¹‹éæŠ•è³‡å‹äººèº«ä¿éšªæ¥­å‹™æ‡‰å…·å‚™è³‡æ ¼æ¢ä»¶åŠæ³¨æ„äº‹é …",
                    "æŠ•è³‡å‹ä¿éšªè§€å¿µ", "æŠ•è³‡å‹ä¿éšªå°ˆè¨­å¸³ç°¿ä¿ç®¡æ©Ÿæ§‹åŠæŠ•è³‡æ¨™çš„æ‡‰æ³¨æ„äº‹é …",
                    "éŠ·å”®æ‡‰æ³¨æ„äº‹é …", "æ–°å‹æ…‹äººèº«ä¿éšªå•†å“å¯©æŸ¥", "ä¿éšªæ¥­å„é¡ç›£æ§æªæ–½"
                ]
            }
        ],
        "default_chapter": "å£½éšªåŸºæœ¬æ¦‚å¿µ"
    }
}

COL_Q = "é¡Œç›®"

# ==========================================
# å·¥å…·é¡åˆ¥ (Client & Logic) - ç¶­æŒé«˜é€Ÿæ‰¹æ¬¡è™•ç†å¼•æ“
# ==========================================

class GeminiClient:
    def __init__(self, api_key):
        self.client = genai.Client(api_key=api_key)
        self.model_name = "gemini-2.5-flash"
        
    def generate(self, prompt, temperature=0.1):
        for attempt in range(5):
            try:
                response = self.client.models.generate_content(
                    model=self.model_name, contents=prompt,
                    config=types.GenerateContentConfig(temperature=temperature, response_mime_type="application/json")
                )
                return response.text.strip()
            except Exception as e:
                wait = (attempt + 1) * 5
                print(f"API Busy, retrying in {wait}s... Error: {e}")
                time.sleep(wait)
        return ""

class ChapterManager:
    def __init__(self, folder_name, pdf_filename, all_chapters, ai_client):
        self.pdf_path = f"{BASE_BANK_DIR}/{folder_name}/{pdf_filename}"
        self.all_chapters = all_chapters
        self.ai = ai_client
        self.full_note_text = ""
        self.chapter_keywords = {} 
        self._read_pdf_from_github()
        self._gen_keywords()

    def _read_pdf_from_github(self):
        data = gh.gh_download_bytes(self.pdf_path)
        if not data: return
        try:
            with pdfplumber.open(BytesIO(data)) as pdf:
                content = [p.extract_text() for p in pdf.pages if p.extract_text()]
                self.full_note_text = "\n".join(content)
        except: pass

    def _gen_keywords(self):
        if not self.full_note_text:
            for ch in self.all_chapters: self.chapter_keywords[ch] = [ch]
            return

        progress_text = "AI æ­£åœ¨é–±è®€ç­†è¨˜å»ºç«‹ç´¢å¼• (åªéœ€åŸ·è¡Œä¸€æ¬¡)..."
        my_bar = st.progress(0, text=progress_text)
        
        prompt = (
            f"ä½ æ˜¯ä¿éšªè€ƒé¡Œåˆ†é¡å°ˆå®¶ã€‚è«‹é–±è®€ç­†è¨˜å¾Œï¼Œé‡å°ä¸‹åˆ—ç« ç¯€ï¼Œå„åˆ—å‡º 5 å€‹æœ€é—œéµçš„å°ˆæœ‰åè©ã€‚\n"
            f"ç« ç¯€åˆ—è¡¨ï¼š{self.all_chapters}\n"
            f"ç­†è¨˜å…§å®¹æ‘˜è¦ï¼š{self.full_note_text[:8000]}...\n\n" # å¢åŠ è®€å–é‡
            f"è«‹å›å‚³ JSON æ ¼å¼ï¼š{{ \"ç« ç¯€å\": [\"é—œéµå­—1\", \"é—œéµå­—2\"...] }}"
        )
        try:
            res = self.ai.generate(prompt)
            data = json.loads(res)
            for ch in self.all_chapters:
                self.chapter_keywords[ch] = data.get(ch, [ch])
        except:
            for ch in self.all_chapters: self.chapter_keywords[ch] = [ch]
            
        my_bar.progress(1.0, text="ç´¢å¼•å»ºç«‹å®Œæˆï¼")
        time.sleep(1)
        my_bar.empty()

class SmartClassifier:
    def __init__(self, mgr, default_ch):
        self.mgr = mgr
        self.default_ch = default_ch

    def classify_batch(self, batch_data):
        """
        æ‰¹æ¬¡åˆ†é¡æ ¸å¿ƒé‚è¼¯
        """
        results = {}
        ai_queue = []

        # 1. é—œéµå­—å¿«é€Ÿç¯©é¸
        for item in batch_data:
            full_text = f"{item['q']} {item['opts']}"
            scores = Counter()
            for ch, kws in self.mgr.chapter_keywords.items():
                for kw in kws:
                    if kw in full_text: 
                        weight = 5 if kw == ch else 1
                        scores[ch] += weight
            
            best, val = scores.most_common(1)[0] if scores else (None, 0)
            
            if val >= 2:
                results[item['id']] = (best, "é—œéµå­—")
            else:
                ai_queue.append(item)

        # 2. AI æ‰¹æ¬¡åˆ¤æ–·
        if ai_queue:
            prompt_items = []
            for item in ai_queue:
                prompt_items.append(f"ID {item['id']}:\né¡Œç›®: {item['q']}\né¸é …: {item['opts']}")
            
            prompt_str = "\n\n".join(prompt_items)
            prompt = (
                f"è«‹å°‡ä¸‹åˆ—é¡Œç›®åˆ†é¡åˆ°æœ€åˆé©çš„ç« ç¯€ã€‚å¯é¸ç« ç¯€ï¼š\n{self.mgr.all_chapters}\n\n"
                f"{prompt_str}\n\n"
                f"è«‹ç›´æ¥å›å‚³ JSON æ ¼å¼ï¼š\n"
                f"[{{ \"id\": \"IDå­—ä¸²\", \"chapter\": \"ç« ç¯€åç¨±\" }}, ...]"
            )
            
            try:
                res_text = self.mgr.ai.generate(prompt)
                res_text = res_text.replace("```json", "").replace("```", "")
                ai_results = json.loads(res_text)
                
                for res in ai_results:
                    res_id = res.get('id')
                    raw_ch = res.get('chapter', self.default_ch)
                    matches = difflib.get_close_matches(raw_ch, self.mgr.all_chapters, n=1, cutoff=0.4)
                    final_ch = matches[0] if matches else self.default_ch
                    results[res_id] = (final_ch, "AIåˆ¤æ–·")
            except Exception as e:
                print(f"Batch AI Failed: {e}")
                for item in ai_queue:
                    results[item['id']] = (self.default_ch, "é è¨­(APIå¤±æ•—)")

        return results

# ==========================================
# å°å¤–ä»‹é¢å‡½æ•¸
# ==========================================

@st.cache_resource(show_spinner=False)
def get_cached_manager(folder_name, note_filename, all_chapters_tuple):
    client = GeminiClient(GEMINI_API_KEY)
    return ChapterManager(folder_name, note_filename, list(all_chapters_tuple), client)

def process_uploaded_file(exam_type, uploaded_file):
    config = EXAM_CONFIGS.get(exam_type)
    if not config: return None

    all_chapters = []
    for output_conf in config['outputs']:
        all_chapters.extend(output_conf['chapters'])

    mgr = get_cached_manager(config['folder'], config['note_file'], tuple(all_chapters))
    classifier = SmartClassifier(mgr, config['default_chapter'])

    try:
        # è®€å– Excelï¼Œé€™è£¡æœƒæ ¹æ“šè¨­å®šè‡ªå‹•å»æ‰¾é¸é …æ¬„ä½
        dfs = pd.read_excel(uploaded_file, sheet_name=None)
    except Exception as e:
        st.error(f"Excel è®€å–å¤±æ•—: {e}")
        return None

    final_results = []
    progress_bar = st.progress(0, text="æº–å‚™é–‹å§‹åˆ†é¡...")
    
    total_rows = sum(len(df) for df in dfs.values() if not df.empty and COL_Q in df.columns)
    processed_count = 0
    BATCH_SIZE = 10 

    for name, df in dfs.items():
        if df.empty or COL_Q not in df.columns: continue
        
        # âš ï¸ é€™è£¡æœƒå‹•æ…‹æ ¹æ“šè€ƒç§‘æŠ“å–æ­£ç¢ºçš„é¸é …æ¬„ä½ (1,2,3 æˆ– ä¸€,äºŒ,ä¸‰)
        valid_opts = [c for c in config['col_opts'] if c in df.columns]
        
        batch_buffer = [] 
        rows_map = {}
        
        for idx, row in df.iterrows():
            q = str(row.get(COL_Q, "")).strip()
            if not q or q.lower() == "nan": continue
            
            opts_txt = " ".join([str(row.get(c, "")) for c in valid_opts])
            
            unique_id = f"{name}_{idx}"
            item = {'id': unique_id, 'q': q, 'opts': opts_txt}
            
            batch_buffer.append(item)
            rows_map[unique_id] = row.to_dict()
            
            if len(batch_buffer) >= BATCH_SIZE or idx == len(df) - 1:
                batch_results = classifier.classify_batch(batch_buffer)
                for item in batch_buffer:
                    res = batch_results.get(item['id'], (config['default_chapter'], "é è¨­"))
                    r = rows_map[item['id']]
                    r["AIåˆ†é¡ç« ç¯€"] = res[0]
                    r["åˆ†é¡ä¾†æº"] = res[1]
                    final_results.append(r)
                
                processed_count += len(batch_buffer)
                progress = min(processed_count / total_rows, 1.0)
                progress_bar.progress(progress, text=f"ğŸ”¥ é«˜é€Ÿåˆ†é¡ä¸­ï¼š{processed_count}/{total_rows} é¡Œ")
                
                batch_buffer = []
                time.sleep(2)

    progress_bar.empty()
    return pd.DataFrame(final_results)

def save_merged_results(exam_type, new_classified_df):
    # æ­¤å‡½å¼ç¶­æŒä¸è®Šï¼Œè² è²¬å°‡çµæœæ¨å› GitHub
    config = EXAM_CONFIGS.get(exam_type)
    base_gh_path = f"{BASE_BANK_DIR}/{config['folder']}"
    logs = []

    for out_conf in config['outputs']:
        filename = out_conf['filename']
        target_chs = out_conf['chapters']
        target_gh_path = f"{base_gh_path}/{filename}"

        sub_new = new_classified_df[new_classified_df["AIåˆ†é¡ç« ç¯€"].isin(target_chs)].copy()
        if sub_new.empty: continue

        existing_df = pd.DataFrame()
        old_file_bytes = gh.gh_download_bytes(target_gh_path)
        if old_file_bytes:
            try:
                xls = pd.read_excel(BytesIO(old_file_bytes), sheet_name=None)
                for sname, sdf in xls.items():
                    if "AIåˆ†é¡ç« ç¯€" not in sdf.columns: sdf["AIåˆ†é¡ç« ç¯€"] = sname
                    existing_df = pd.concat([existing_df, sdf], ignore_index=True)
            except: pass

        if not existing_df.empty:
            common = list(set(existing_df.columns) & set(sub_new.columns))
            if COL_Q in common:
                combined = pd.concat([existing_df, sub_new], ignore_index=True)
            else:
                combined = sub_new
        else:
            combined = sub_new
        
        before = len(combined)
        combined.drop_duplicates(subset=[COL_Q], keep='last', inplace=True)
        after = len(combined)
        
        logs.append(f"ğŸ“„ **{filename}**ï¼šæ–°å¢ {len(sub_new)} é¡Œï¼Œåˆä½µå¾Œå…± {after} é¡Œ (å·²å»é‡)ã€‚")

        mapper = {name: i for i, name in enumerate(target_chs)}
        combined["Sort"] = combined["AIåˆ†é¡ç« ç¯€"].map(mapper).fillna(999)
        combined = combined.sort_values("Sort")

        output = BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            for ch in target_chs:
                ch_df = combined[combined["AIåˆ†é¡ç« ç¯€"] == ch]
                if not ch_df.empty:
                    safe = ch.replace("/", "_")[:30]
                    ch_df.drop(columns=["Sort"], errors="ignore").to_excel(writer, sheet_name=safe, index=False)
        
        gh.gh_put_file(target_gh_path, output.getvalue(), f"Auto-Merge: {filename}")

    return logs