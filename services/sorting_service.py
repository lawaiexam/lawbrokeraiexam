import pandas as pd
import os
import re
import time
import pdfplumber
from io import BytesIO
from collections import Counter
from google import genai
from google.genai import types
import streamlit as st
from utils import github_handler as gh  # å¼•å…¥ GitHub å·¥å…·

# ==========================================
# è¨­å®šå€
# ==========================================

# ç­†è¨˜è·¯å¾‘ï¼šå‡è¨­ç­†è¨˜æ”¾åœ¨ bank/{folder}/ ä¸‹
BASE_BANK_DIR = "bank"

# API Key
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "AIzaSyCiNkDK8pfn305ZSlHmWbVj89_sXBl2eqo")

EXAM_CONFIGS = {
    "äººèº«ä¿éšª": {
        "folder": "äººèº«",
        "note_file": "ç­†è¨˜_äººèº«.pdf",
        "col_opts": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"], 
        "outputs": [
            {
                "filename": "äººèº«_ä¿éšªæ³•è¦.xlsx",
                "chapters": [
                    "ä¿éšªä¸­é‡è¦çš„è§’è‰²", "ä¿éšªå¥‘ç´„", "ä¿éšªå¥‘ç´„å…­å¤§åŸå‰‡", "å¥‘ç´„è§£é™¤ã€ç„¡æ•ˆã€å¤±æ•ˆã€åœæ•ˆã€å¾©æ•ˆ",
                    "ä¿éšªé‡‘èˆ‡è§£ç´„é‡‘", "ç¹¼æ‰¿ç›¸é—œ", "éºç”¢ç¨…ã€è´ˆèˆ‡ç¨…", "æ‰€å¾—ç¨…",
                    "ä¿éšªæ¥­å‹™å“¡ç›¸é—œæ³•è¦åŠè¦å®š", "é‡‘èæ¶ˆè²»è€…ä¿è­·æ³•", "å€‹äººè³‡æ–™ä¿è­·æ³•", "æ´—éŒ¢é˜²åˆ¶æ³•"
                ]
            },
            {
                "filename": "äººèº«_ä¿éšªå¯¦å‹™.xlsx",
                "chapters": [
                    "é¢¨éšªèˆ‡é¢¨éšªç®¡ç†", "äººèº«ä¿éšªæ­·å²åŠç”Ÿå‘½è¡¨", "ä¿éšªè²»æ¶æ§‹ã€è§£ç´„é‡‘ã€æº–å‚™é‡‘ã€ä¿å–®ç´…åˆ©",
                    "äººèº«ä¿éšªæ„ç¾©ã€åŠŸèƒ½ã€åˆ†é¡", "äººèº«ä¿éšªï¼äººå£½ä¿éšª", "äººèº«ä¿éšªï¼å¹´é‡‘ä¿éšª",
                    "äººèº«ä¿éšªï¼å¥åº·ä¿éšª", "äººèº«ä¿éšªï¼å‚·å®³ä¿éšª", "äººèº«ä¿éšªï¼å…¶ä»–äººèº«ä¿éšª", "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·"
                ]
            }
        ],
        "default_chapter": "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·"
    },
    "æŠ•è³‡å‹ä¿éšª": {
        "folder": "æŠ•è³‡å‹",
        "note_file": "ç­†è¨˜_æŠ•è³‡å‹.pdf",
        "col_opts": ["é¸é …1", "é¸é …2", "é¸é …3", "é¸é …4"], 
        "outputs": [
            {
                "filename": "æŠ•è³‡å‹_æ³•ä»¤è¦ç« .xlsx",
                "chapters": [
                    "æŠ•è³‡å‹ä¿éšªæ¦‚è«–", "æŠ•è³‡å‹ä¿éšªæ³•ä»¤ä»‹ç´¹", "é‡‘èé«”ç³»æ¦‚è¿°", "è­‰åˆ¸æŠ•è³‡ä¿¡è¨—åŠé¡§å•ä¹‹è¦ç¯„èˆ‡åˆ¶åº¦"
                ]
            },
            {
                "filename": "æŠ•è³‡å‹_æŠ•è³‡å¯¦å‹™.xlsx",
                "chapters": [
                    "è²¨å¹£æ™‚é–“åƒ¹å€¼", "å‚µåˆ¸è©•åƒ¹", "è­‰åˆ¸è©•åƒ¹", "é¢¨éšªã€å ±é…¬èˆ‡æŠ•è³‡çµ„åˆ",
                    "è³‡æœ¬è³‡ç”¢è¨‚åƒ¹æ¨¡å¼ã€ç¸¾æ•ˆ", "æŠ•è³‡å·¥å…·ç°¡ä»‹"
                ]
            }
        ],
        "default_chapter": "æŠ•è³‡å‹ä¿éšªæ¦‚è«–"
    },
    "å¤–å¹£ä¿å–®": {
        "folder": "å¤–å¹£",
        "note_file": "ç­†è¨˜_å¤–å¹£.pdf",
        "col_opts": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"],
        "outputs": [
            {
                "filename": "å¤–å¹£.xlsx", 
                "chapters": [
                    "å£½éšªåŸºæœ¬æ¦‚å¿µ", "ä¿éšªæ¥­è¾¦ç†å¤–åŒ¯æ¥­å‹™ç®¡ç†è¾¦æ³•", "ç®¡ç†å¤–åŒ¯æ¢ä¾‹", "å¤–åŒ¯æ”¶æ”¯æˆ–äº¤æ˜“ç”³å ±è¾¦æ³•",
                    "ä¿éšªæ¥­è¾¦ç†åœ‹å¤–æŠ•è³‡ç®¡ç†è¾¦æ³•", "äººèº«ä¿éšªæ¥­è¾¦ç†ä»¥å¤–å¹£æ”¶ä»˜ä¹‹éæŠ•è³‡å‹äººèº«ä¿éšªæ¥­å‹™æ‡‰å…·å‚™è³‡æ ¼æ¢ä»¶åŠæ³¨æ„äº‹é …",
                    "æŠ•è³‡å‹ä¿éšªè§€å¿µ", "æŠ•è³‡å‹ä¿éšªå°ˆè¨­å¸³ç°¿ä¿ç®¡æ©Ÿæ§‹åŠæŠ•è³‡æ¨™çš„æ‡‰æ³¨æ„äº‹é …",
                    "éŠ·å”®æ‡‰æ³¨æ„äº‹é …", "æ–°å‹æ…‹äººèº«ä¿éšªå•†å“å¯©æŸ¥", "ä¿éšªæ¥­å„é¡ç›£æ§æªæ–½"
                ]
            }
        ],
        "default_chapter": "å£½éšªåŸºæœ¬æ¦‚å¿µ"
    }
}

COL_Q = "é¡Œç›®"

# ==========================================
# å·¥å…·é¡åˆ¥ (Client & Logic)
# ==========================================

class GeminiClient:
    def __init__(self, api_key):
        self.client = genai.Client(api_key=api_key)
        self.model_name = "gemini-2.5-flash"
        
    def generate(self, prompt, temperature=0.1):
        for attempt in range(3):
            try:
                response = self.client.models.generate_content(
                    model=self.model_name, contents=prompt,
                    config=types.GenerateContentConfig(temperature=temperature)
                )
                return response.text.strip()
            except Exception:
                time.sleep((attempt + 1) * 2)
        return ""

class ChapterManager:
    def __init__(self, folder_name, pdf_filename, all_chapters, ai_client):
        # é›²ç«¯ç‰ˆï¼šå˜—è©¦å¾ GitHub ä¸‹è¼‰ç­†è¨˜å…§å®¹
        self.pdf_path = f"{BASE_BANK_DIR}/{folder_name}/{pdf_filename}"
        self.all_chapters = all_chapters
        self.ai = ai_client
        self.full_note_text = ""
        self.chapter_keywords = {} 
        self._read_pdf_from_github()
        self._gen_keywords()

    def _read_pdf_from_github(self):
        # ä½¿ç”¨ gh_download_bytes è®€å–ç­†è¨˜
        data = gh.gh_download_bytes(self.pdf_path)
        if not data:
            return
        
        try:
            with pdfplumber.open(BytesIO(data)) as pdf:
                content = []
                for page in pdf.pages:
                    txt = page.extract_text()
                    if txt: content.append(txt)
                self.full_note_text = "\n".join(content)
        except Exception:
            pass

    def _get_context(self, chapter):
        if not self.full_note_text: return ""
        terms = re.split(r'[ã€\s\-\(\)è¾¦æ³•æ³¨æ„äº‹é …]', chapter)
        terms = [t for t in terms if len(t) >= 2]
        terms.append(chapter) 
        snippets = []
        for t in terms:
            indices = [m.start() for m in re.finditer(re.escape(t), self.full_note_text)]
            for idx in indices[:2]:
                snippets.append(self.full_note_text[max(0, idx-200):min(len(self.full_note_text), idx+500)])
        return "\n...\n".join(snippets)

    def _gen_keywords(self):
        if not self.full_note_text:
            for ch in self.all_chapters:
                self.chapter_keywords[ch] = [ch]
            return

        progress_text = "æ­£åœ¨åˆ†æç­†è¨˜èˆ‡å»ºç«‹é—œéµå­—..."
        my_bar = st.progress(0, text=progress_text)
        
        total = len(self.all_chapters)
        for i, ch in enumerate(self.all_chapters):
            ctx = self._get_context(ch)
            prompt = (
                f"ä½ æ˜¯ä¿éšªè€ƒé¡Œåˆ†é¡å°ˆå®¶ã€‚è«‹é‡å°ç« ç¯€ã€{ch}ã€ï¼Œåˆ—å‡º 5-10 å€‹æ ¸å¿ƒã€Œå°ˆæœ‰åè©ã€æˆ–ã€Œé—œéµå­—ã€ã€‚\n"
                f"åƒè€ƒç­†è¨˜ï¼š\n{ctx}\nåªè¼¸å‡ºé—œéµå­—ï¼Œç”¨é€—è™Ÿåˆ†éš”ã€‚"
            )
            res = self.ai.generate(prompt)
            if res:
                kws = [k.strip() for k in res.replace("ã€", ",").replace("\n", ",").split(",") if len(k.strip())>1]
                self.chapter_keywords[ch] = kws[:15]
            else:
                self.chapter_keywords[ch] = [ch]
            
            my_bar.progress((i + 1) / total, text=f"åˆ†æç« ç¯€ï¼š{ch}")
            time.sleep(0.2)
        
        my_bar.empty()

class SmartClassifier:
    def __init__(self, mgr, default_ch):
        self.mgr = mgr
        self.default_ch = default_ch
        self.chapters_str = "\n".join([f"- {c}" for c in mgr.all_chapters])

    def classify(self, q, opts):
        full = f"{q} {opts}"
        # Rule-Based
        scores = Counter()
        for ch, kws in self.mgr.chapter_keywords.items():
            for kw in kws:
                if kw in full: scores[ch] += (5 if kw == ch else 1)
        if scores:
            best, val = scores.most_common(1)[0]
            if val >= 2: return best, "é—œéµå­—"

        # AI-Based
        prompt = (
            f"é¡Œç›®ï¼š{q}\né¸é …ï¼š{opts}\n"
            f"è«‹å¾ä¸‹åˆ—ç« ç¯€é¸å‡ºæœ€åˆé©çš„ä¸€å€‹ï¼š\n{self.chapters_str}\n"
            f"åªè¼¸å‡ºç« ç¯€åç¨±ã€‚è‹¥ä¸ç¢ºå®šè«‹è¼¸å‡ºã€Œ{self.default_ch}ã€ã€‚"
        )
        ans = self.mgr.ai.generate(prompt)
        for ch in self.mgr.all_chapters:
            if ch in ans: return ch, "AIåˆ¤æ–·"
        return self.default_ch, "é è¨­"

# ==========================================
# å°å¤–ä»‹é¢å‡½æ•¸
# ==========================================
@st.cache_resource(show_spinner=False)
def get_cached_manager(folder_name, note_filename, all_chapters_tuple):
    # é€™è£¡å¿…é ˆæŠŠ list è½‰æˆ tuple æ‰èƒ½è¢« cacheï¼Œè£¡é¢å†è½‰å› list
    client = GeminiClient(GEMINI_API_KEY)
    return ChapterManager(folder_name, note_filename, list(all_chapters_tuple), client)

def process_uploaded_file(exam_type, uploaded_file):
    config = EXAM_CONFIGS.get(exam_type)
    if not config: return None

    all_chapters = []
    for output_conf in config['outputs']:
        all_chapters.extend(output_conf['chapters'])

    # ğŸ‘‡ã€ä¿®æ”¹ã€‘åŸæœ¬æ˜¯ç›´æ¥ new ChapterManagerï¼Œç¾åœ¨æ”¹å‘¼å«ä¸Šé¢çš„å¿«å–å‡½å¼
    # æ³¨æ„ï¼šæˆ‘å€‘æŠŠ all_chapters (list) è½‰æˆ tuple å‚³é€²å»ï¼Œå› ç‚º list ä¸èƒ½è¢«é›œæ¹Š(hash)
    mgr = get_cached_manager(config['folder'], config['note_file'], tuple(all_chapters))
    
    classifier = SmartClassifier(mgr, config['default_chapter'])
    
def process_uploaded_file(exam_type, uploaded_file):
    config = EXAM_CONFIGS.get(exam_type)
    if not config: return None

    all_chapters = []
    for output_conf in config['outputs']:
        all_chapters.extend(output_conf['chapters'])

    client = GeminiClient(GEMINI_API_KEY)
    # å‚³å…¥ folder åç¨±ä»¥ä¾¿å¾ GitHub è®€å–ç­†è¨˜
    mgr = ChapterManager(config['folder'], config['note_file'], all_chapters, client)
    classifier = SmartClassifier(mgr, config['default_chapter'])

    try:
        dfs = pd.read_excel(uploaded_file, sheet_name=None)
    except Exception as e:
        st.error(f"Excel è®€å–å¤±æ•—: {e}")
        return None

    results = []
    total_sheets = len(dfs)
    curr_sheet = 0
    progress_bar = st.progress(0, text="é–‹å§‹åˆ†é¡é¡Œç›®...")

    for name, df in dfs.items():
        curr_sheet += 1
        if df.empty or COL_Q not in df.columns: 
            continue
        
        valid_opts = [c for c in config['col_opts'] if c in df.columns]
        total_rows = len(df)
        
        for idx, row in df.iterrows():
            q = str(row.get(COL_Q, "")).strip()
            if not q or q.lower() == "nan": continue
            
            opts_txt = " ".join([str(row.get(c, "")) for c in valid_opts])
            ch, src = classifier.classify(q, opts_txt)
            
            r = row.to_dict()
            r["AIåˆ†é¡ç« ç¯€"] = ch
            r["åˆ†é¡ä¾†æº"] = src
            results.append(r)
            
            if idx % 5 == 0:
                progress = (idx + 1) / total_rows
                progress_bar.progress(progress, text=f"æ­£åœ¨è™•ç†åˆ†é  '{name}'ï¼šç¬¬ {idx+1}/{total_rows} é¡Œ")

    progress_bar.empty()
    return pd.DataFrame(results)

def save_merged_results(exam_type, new_classified_df):
    """
    å°‡åˆ†é¡å¥½çš„ DF åˆä½µå› GitHub ä¸Šçš„é¡Œåº«
    """
    config = EXAM_CONFIGS.get(exam_type)
    # GitHub ä¸Šçš„è³‡æ–™å¤¾è·¯å¾‘ (ä¾‹å¦‚ bank/äººèº«)
    base_gh_path = f"{BASE_BANK_DIR}/{config['folder']}"
    
    logs = []

    for out_conf in config['outputs']:
        filename = out_conf['filename']
        target_chs = out_conf['chapters']
        # GitHub å®Œæ•´æª”æ¡ˆè·¯å¾‘
        target_gh_path = f"{base_gh_path}/{filename}"

        # ç¯©é¸æ–°é¡Œç›®
        sub_new = new_classified_df[new_classified_df["AIåˆ†é¡ç« ç¯€"].isin(target_chs)].copy()
        if sub_new.empty:
            continue

        # 1. å˜—è©¦å¾ GitHub ä¸‹è¼‰èˆŠæª”
        existing_df = pd.DataFrame()
        old_file_bytes = gh.gh_download_bytes(target_gh_path)
        
        if old_file_bytes:
            try:
                xls = pd.read_excel(BytesIO(old_file_bytes), sheet_name=None)
                for sname, sdf in xls.items():
                    if "AIåˆ†é¡ç« ç¯€" not in sdf.columns:
                        sdf["AIåˆ†é¡ç« ç¯€"] = sname
                    existing_df = pd.concat([existing_df, sdf], ignore_index=True)
            except Exception:
                pass

        # 2. åˆä½µèˆ‡å»é‡
        if not existing_df.empty:
            common = list(set(existing_df.columns) & set(sub_new.columns))
            if COL_Q in common:
                combined = pd.concat([existing_df, sub_new], ignore_index=True)
            else:
                combined = sub_new
        else:
            combined = sub_new
        
        before = len(combined)
        combined.drop_duplicates(subset=[COL_Q], keep='last', inplace=True)
        after = len(combined)
        removed = before - after
        
        logs.append(f"ğŸ“„ **{filename}**ï¼šæ–°å¢ {len(sub_new)} é¡Œï¼Œåˆä½µå¾Œå…± {after} é¡Œ (å·²è‡ªå‹•ç§»é™¤ {removed} é¡Œé‡è¤‡)ã€‚")

        # 3. è½‰å­˜ç‚º Excel Bytes
        mapper = {name: i for i, name in enumerate(target_chs)}
        combined["Sort"] = combined["AIåˆ†é¡ç« ç¯€"].map(mapper).fillna(999)
        combined = combined.sort_values("Sort")

        output = BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            for ch in target_chs:
                ch_df = combined[combined["AIåˆ†é¡ç« ç¯€"] == ch]
                if not ch_df.empty:
                    safe = ch.replace("/", "_")[:30]
                    ch_df.drop(columns=["Sort"], errors="ignore").to_excel(writer, sheet_name=safe, index=False)
        
        # 4. ä¸Šå‚³å› GitHub (è¦†è“‹èˆŠæª”)
        file_bytes = output.getvalue()
        gh.gh_put_file(
            target_gh_path, 
            file_bytes, 
            f"Auto-Merge: Updated {filename} via Admin Panel"
        )

    return logs