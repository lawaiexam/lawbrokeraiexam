import pandas as pd
import os
import time
import json
import difflib
from io import BytesIO
from collections import Counter
from google import genai
from google.genai import types
import streamlit as st
from utils import github_handler as gh

# ==========================================
# è¨­å®šå€
# ==========================================
BASE_BANK_DIR = "bank"
KEYWORDS_FILE = "keywords_db.json"
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")

EXAM_CONFIGS = {
    "äººèº«ä¿éšª": {
        "folder": "äººèº«",
        "note_file": "ç­†è¨˜_äººèº«.pdf",
        "col_opts": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"], 
        "outputs": [
            {
                "filename": "äººèº«_ä¿éšªæ³•è¦.xlsx",
                "chapters": [
                    "ä¿éšªä¸­é‡è¦çš„è§’è‰²", "ä¿éšªå¥‘ç´„", "ä¿éšªå¥‘ç´„å…­å¤§åŸå‰‡", "å¥‘ç´„è§£é™¤ã€ç„¡æ•ˆã€å¤±æ•ˆã€åœæ•ˆã€å¾©æ•ˆ",
                    "ä¿éšªé‡‘èˆ‡è§£ç´„é‡‘", "ç¹¼æ‰¿ç›¸é—œ", "éºç”¢ç¨…ã€è´ˆèˆ‡ç¨…", "æ‰€å¾—ç¨…",
                    "ä¿éšªæ¥­å‹™å“¡ç›¸é—œæ³•è¦åŠè¦å®š", "é‡‘èæ¶ˆè²»è€…ä¿è­·æ³•", "å€‹äººè³‡æ–™ä¿è­·æ³•", "æ´—éŒ¢é˜²åˆ¶æ³•"
                ]
            },
            {
                "filename": "äººèº«_ä¿éšªå¯¦å‹™.xlsx",
                "chapters": [
                    "é¢¨éšªèˆ‡é¢¨éšªç®¡ç†", "äººèº«ä¿éšªæ­·å²åŠç”Ÿå‘½è¡¨", "ä¿éšªè²»æ¶æ§‹ã€è§£ç´„é‡‘ã€æº–å‚™é‡‘ã€ä¿å–®ç´…åˆ©",
                    "äººèº«ä¿éšªæ„ç¾©ã€åŠŸèƒ½ã€åˆ†é¡", "äººèº«ä¿éšªï¼äººå£½ä¿éšª", "äººèº«ä¿éšªï¼å¹´é‡‘ä¿éšª",
                    "äººèº«ä¿éšªï¼å¥åº·ä¿éšª", "äººèº«ä¿éšªï¼å‚·å®³ä¿éšª", "äººèº«ä¿éšªï¼å…¶ä»–äººèº«ä¿éšª", "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·"
                ]
            }
        ],
        "default_chapter": "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·"
    },
    "æŠ•è³‡å‹ä¿éšª": {
        "folder": "æŠ•è³‡å‹",
        "note_file": "ç­†è¨˜_æŠ•è³‡å‹.pdf",
        "col_opts": ["é¸é …1", "é¸é …2", "é¸é …3", "é¸é …4"], 
        "outputs": [
            {
                "filename": "æŠ•è³‡å‹_æ³•ä»¤è¦ç« .xlsx",
                "chapters": [
                    "æŠ•è³‡å‹ä¿éšªæ¦‚è«–", "æŠ•è³‡å‹ä¿éšªæ³•ä»¤ä»‹ç´¹", "é‡‘èé«”ç³»æ¦‚è¿°", "è­‰åˆ¸æŠ•è³‡ä¿¡è¨—åŠé¡§å•ä¹‹è¦ç¯„èˆ‡åˆ¶åº¦"
                ]
            },
            {
                "filename": "æŠ•è³‡å‹_æŠ•è³‡å¯¦å‹™.xlsx",
                "chapters": [
                    "è²¨å¹£æ™‚é–“åƒ¹å€¼", "å‚µåˆ¸è©•åƒ¹", "è­‰åˆ¸è©•åƒ¹", "é¢¨éšªã€å ±é…¬èˆ‡æŠ•è³‡çµ„åˆ",
                    "è³‡æœ¬è³‡ç”¢è¨‚åƒ¹æ¨¡å¼ã€ç¸¾æ•ˆ", "æŠ•è³‡å·¥å…·ç°¡ä»‹"
                ]
            }
        ],
        "default_chapter": "æŠ•è³‡å‹ä¿éšªæ¦‚è«–"
    },
    "å¤–å¹£ä¿å–®": {
        "folder": "å¤–å¹£",
        "note_file": "ç­†è¨˜_å¤–å¹£.pdf",
        "col_opts": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"],
        "outputs": [
            {
                "filename": "å¤–å¹£.xlsx", 
                "chapters": [
                    "å£½éšªåŸºæœ¬æ¦‚å¿µ", "ä¿éšªæ¥­è¾¦ç†å¤–åŒ¯æ¥­å‹™ç®¡ç†è¾¦æ³•", "ç®¡ç†å¤–åŒ¯æ¢ä¾‹", "å¤–åŒ¯æ”¶æ”¯æˆ–äº¤æ˜“ç”³å ±è¾¦æ³•",
                    "ä¿éšªæ¥­è¾¦ç†åœ‹å¤–æŠ•è³‡ç®¡ç†è¾¦æ³•", "äººèº«ä¿éšªæ¥­è¾¦ç†ä»¥å¤–å¹£æ”¶ä»˜ä¹‹éæŠ•è³‡å‹äººèº«ä¿éšªæ¥­å‹™æ‡‰å…·å‚™è³‡æ ¼æ¢ä»¶åŠæ³¨æ„äº‹é …",
                    "æŠ•è³‡å‹ä¿éšªè§€å¿µ", "æŠ•è³‡å‹ä¿éšªå°ˆè¨­å¸³ç°¿ä¿ç®¡æ©Ÿæ§‹åŠæŠ•è³‡æ¨™çš„æ‡‰æ³¨æ„äº‹é …",
                    "éŠ·å”®æ‡‰æ³¨æ„äº‹é …", "æ–°å‹æ…‹äººèº«ä¿éšªå•†å“å¯©æŸ¥", "ä¿éšªæ¥­å„é¡ç›£æ§æªæ–½"
                ]
            }
        ],
        "default_chapter": "å£½éšªåŸºæœ¬æ¦‚å¿µ"
    }
}

COL_Q = "é¡Œç›®"

# ==========================================
# å·¥å…·é¡åˆ¥ (Client & Logic)
# ==========================================

class GeminiClient:
    def __init__(self, api_key):
        self.client = genai.Client(api_key=api_key)
        self.model_name = "gemini-2.5-flash"
        
    def generate(self, prompt, temperature=0.1):
        for attempt in range(5):
            try:
                response = self.client.models.generate_content(
                    model=self.model_name, contents=prompt,
                    config=types.GenerateContentConfig(temperature=temperature, response_mime_type="application/json")
                )
                return response.text.strip()
            except Exception as e:
                wait = (attempt + 1) * 5
                print(f"API Busy (Error: {e}), retrying in {wait}s...")
                time.sleep(wait)
        return ""

class ChapterManager:
    def __init__(self, exam_type, all_chapters, ai_client):
        self.exam_type = exam_type
        self.all_chapters = all_chapters
        self.ai = ai_client
        self.chapter_keywords = {} 
        self._load_static_keywords()

    def _load_static_keywords(self):
        if os.path.exists(KEYWORDS_FILE):
            try:
                with open(KEYWORDS_FILE, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if self.exam_type in data:
                        self.chapter_keywords = data[self.exam_type]
                        for ch in self.all_chapters:
                            if ch not in self.chapter_keywords:
                                self.chapter_keywords[ch] = [ch]
                        return
            except Exception as e:
                print(f"è®€å–é—œéµå­—æª”å¤±æ•—: {e}")
        
        # Fallback
        for ch in self.all_chapters:
            self.chapter_keywords[ch] = [ch]

class SmartClassifier:
    def __init__(self, mgr, default_ch):
        self.mgr = mgr
        self.default_ch = default_ch

    def classify_batch(self, batch_data):
        results = {}
        ai_queue = []

        # 1. é—œéµå­—å¿«é€Ÿç¯©é¸
        for item in batch_data:
            full_text = f"{item['q']} {item['opts']}"
            scores = Counter()
            for ch, kws in self.mgr.chapter_keywords.items():
                for kw in kws:
                    if kw in full_text: 
                        weight = 5 if kw == ch else 1
                        scores[ch] += weight
            
            best, val = scores.most_common(1)[0] if scores else (None, 0)
            
            if val >= 2:
                results[item['id']] = (best, "é—œéµå­—")
            else:
                ai_queue.append(item)

        # 2. AI æ‰¹æ¬¡åˆ¤æ–·
        if ai_queue:
            prompt_items = []
            for item in ai_queue:
                prompt_items.append(f"ID {item['id']}:\né¡Œç›®: {item['q']}\né¸é …: {item['opts']}")
            
            prompt_str = "\n\n".join(prompt_items)
            prompt = (
                f"è«‹å°‡ä¸‹åˆ—é¡Œç›®åˆ†é¡åˆ°æœ€åˆé©çš„ç« ç¯€ã€‚å¯é¸ç« ç¯€ï¼š\n{self.mgr.all_chapters}\n\n"
                f"{prompt_str}\n\n"
                f"è«‹ç›´æ¥å›å‚³ JSON æ ¼å¼ï¼š\n"
                f"[{{ \"id\": \"IDå­—ä¸²\", \"chapter\": \"ç« ç¯€åç¨±\" }}, ...]"
            )
            
            try:
                res_text = self.mgr.ai.generate(prompt)
                res_text = res_text.replace("```json", "").replace("```", "")
                ai_results = json.loads(res_text)
                
                for res in ai_results:
                    res_id = res.get('id')
                    raw_ch = res.get('chapter', self.default_ch)
                    matches = difflib.get_close_matches(raw_ch, self.mgr.all_chapters, n=1, cutoff=0.4)
                    final_ch = matches[0] if matches else self.default_ch
                    results[res_id] = (final_ch, "AIåˆ¤æ–·")
            except Exception as e:
                print(f"Batch AI Failed: {e}")
                for item in ai_queue:
                    results[item['id']] = (self.default_ch, "é è¨­(APIå¤±æ•—)")

        return results

# ==========================================
# å°å¤–ä»‹é¢å‡½æ•¸ (Process & Save)
# ==========================================

@st.cache_resource(show_spinner=False)
def get_cached_manager(exam_type, all_chapters_tuple):
    client = GeminiClient(GEMINI_API_KEY)
    return ChapterManager(exam_type, list(all_chapters_tuple), client)

def process_uploaded_file(exam_type, uploaded_file):
    config = EXAM_CONFIGS.get(exam_type)
    if not config: return None

    all_chapters = []
    for output_conf in config['outputs']:
        all_chapters.extend(output_conf['chapters'])

    mgr = get_cached_manager(exam_type, tuple(all_chapters))
    classifier = SmartClassifier(mgr, config['default_chapter'])

    try:
        dfs = pd.read_excel(uploaded_file, sheet_name=None)
    except Exception as e:
        st.error(f"Excel è®€å–å¤±æ•—: {e}")
        return None

    final_results = []
    progress_bar = st.progress(0, text="æº–å‚™é–‹å§‹åˆ†é¡...")
    
    total_rows = sum(len(df) for df in dfs.values() if not df.empty and COL_Q in df.columns)
    processed_count = 0
    BATCH_SIZE = 10 

    for name, df in dfs.items():
        if df.empty or COL_Q not in df.columns: continue
        
        valid_opts = [c for c in config['col_opts'] if c in df.columns]
        
        batch_buffer = [] 
        rows_map = {}
        
        for idx, row in df.iterrows():
            q = str(row.get(COL_Q, "")).strip()
            if not q or q.lower() == "nan": continue
            
            opts_txt = " ".join([str(row.get(c, "")) for c in valid_opts])
            
            unique_id = f"{name}_{idx}"
            item = {'id': unique_id, 'q': q, 'opts': opts_txt}
            
            batch_buffer.append(item)
            rows_map[unique_id] = row.to_dict()
            
            if len(batch_buffer) >= BATCH_SIZE or idx == len(df) - 1:
                batch_results = classifier.classify_batch(batch_buffer)
                for item in batch_buffer:
                    res = batch_results.get(item['id'], (config['default_chapter'], "é è¨­"))
                    r = rows_map[item['id']]
                    r["AIåˆ†é¡ç« ç¯€"] = res[0]
                    r["åˆ†é¡ä¾†æº"] = res[1]
                    final_results.append(r)
                
                processed_count += len(batch_buffer)
                progress = min(processed_count / total_rows, 1.0)
                progress_bar.progress(progress, text=f"ğŸ”¥ é«˜é€Ÿåˆ†é¡ä¸­ï¼š{processed_count}/{total_rows} é¡Œ")
                
                batch_buffer = []
                time.sleep(2)

    progress_bar.empty()
    return pd.DataFrame(final_results)

# ğŸ‘‡ ã€æ ¸å¿ƒä¿®æ­£ã€‘ï¼šåŠ å…¥ä¸Šå‚³ç‹€æ…‹æª¢æŸ¥èˆ‡éŒ¯èª¤æ””æˆª
def save_merged_results(exam_type, new_classified_df):
    config = EXAM_CONFIGS.get(exam_type)
    base_gh_path = f"{BASE_BANK_DIR}/{config['folder']}"
    logs = []

    for out_conf in config['outputs']:
        filename = out_conf['filename']
        target_chs = out_conf['chapters']
        target_gh_path = f"{base_gh_path}/{filename}"

        sub_new = new_classified_df[new_classified_df["AIåˆ†é¡ç« ç¯€"].isin(target_chs)].copy()
        if sub_new.empty: continue

        existing_df = pd.DataFrame()
        old_file_bytes = gh.gh_download_bytes(target_gh_path)
        if old_file_bytes:
            try:
                xls = pd.read_excel(BytesIO(old_file_bytes), sheet_name=None)
                for sname, sdf in xls.items():
                    if "AIåˆ†é¡ç« ç¯€" not in sdf.columns: sdf["AIåˆ†é¡ç« ç¯€"] = sname
                    existing_df = pd.concat([existing_df, sdf], ignore_index=True)
            except: pass

        if not existing_df.empty:
            common = list(set(existing_df.columns) & set(sub_new.columns))
            if COL_Q in common:
                combined = pd.concat([existing_df, sub_new], ignore_index=True)
            else:
                combined = sub_new
        else:
            combined = sub_new
        
        combined.drop_duplicates(subset=[COL_Q], keep='last', inplace=True)
        after = len(combined)
        
        mapper = {name: i for i, name in enumerate(target_chs)}
        combined["Sort"] = combined["AIåˆ†é¡ç« ç¯€"].map(mapper).fillna(999)
        combined = combined.sort_values("Sort")

        output = BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            for ch in target_chs:
                ch_df = combined[combined["AIåˆ†é¡ç« ç¯€"] == ch]
                if not ch_df.empty:
                    safe = ch.replace("/", "_")[:30]
                    ch_df.drop(columns=["Sort"], errors="ignore").to_excel(writer, sheet_name=safe, index=False)
        
        # ğŸ‘‡ é€™è£¡æ”¹äº†ï¼šå˜—è©¦ä¸Šå‚³ï¼Œä¸¦æ ¹æ“šçµæœå¯« log
        try:
            result = gh.gh_put_file(target_gh_path, output.getvalue(), f"Auto-Merge: {filename}")
            
            # å¦‚æœä¸Šå‚³æˆåŠŸ (result ä¸æ˜¯ False)
            if result is not False:
                logs.append(f"âœ… **{filename}**ï¼šæˆåŠŸä¸Šå‚³ï¼æ›´æ–°å¾Œå…± {after} é¡Œã€‚")
            else:
                logs.append(f"âŒ **{filename}**ï¼šä¸Šå‚³å¤±æ•— (GitHub Token æ¬Šé™ä¸è¶³æˆ–ç„¡æ•ˆ)ã€‚")
        except Exception as e:
            logs.append(f"âŒ **{filename}**ï¼šä¸Šå‚³ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼åŸå› ï¼š{str(e)}")

    return logs