import pandas as pd
import os
import re
import time
import json
import difflib
import pdfplumber
from io import BytesIO
from collections import Counter
from google import genai
from google.genai import types
import streamlit as st
from utils import github_handler as gh

# ==========================================
# è¨­å®šå€
# ==========================================
BASE_BANK_DIR = "bank"
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")

EXAM_CONFIGS = {
    "äººèº«ä¿éšª": {
        "folder": "äººèº«",
        "note_file": "ç­†è¨˜_äººèº«.pdf",
        "col_opts": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"], 
        "outputs": [
            {
                "filename": "äººèº«_ä¿éšªæ³•è¦.xlsx",
                "chapters": [
                    "ä¿éšªä¸­é‡è¦çš„è§’è‰²", "ä¿éšªå¥‘ç´„", "ä¿éšªå¥‘ç´„å…­å¤§åŸå‰‡", "å¥‘ç´„è§£é™¤ã€ç„¡æ•ˆã€å¤±æ•ˆã€åœæ•ˆã€å¾©æ•ˆ",
                    "ä¿éšªé‡‘èˆ‡è§£ç´„é‡‘", "ç¹¼æ‰¿ç›¸é—œ", "éºç”¢ç¨…ã€è´ˆèˆ‡ç¨…", "æ‰€å¾—ç¨…",
                    "ä¿éšªæ¥­å‹™å“¡ç›¸é—œæ³•è¦åŠè¦å®š", "é‡‘èæ¶ˆè²»è€…ä¿è­·æ³•", "å€‹äººè³‡æ–™ä¿è­·æ³•", "æ´—éŒ¢é˜²åˆ¶æ³•"
                ]
            },
            {
                "filename": "äººèº«_ä¿éšªå¯¦å‹™.xlsx",
                "chapters": [
                    "é¢¨éšªèˆ‡é¢¨éšªç®¡ç†", "äººèº«ä¿éšªæ­·å²åŠç”Ÿå‘½è¡¨", "ä¿éšªè²»æ¶æ§‹ã€è§£ç´„é‡‘ã€æº–å‚™é‡‘ã€ä¿å–®ç´…åˆ©",
                    "äººèº«ä¿éšªæ„ç¾©ã€åŠŸèƒ½ã€åˆ†é¡", "äººèº«ä¿éšªï¼äººå£½ä¿éšª", "äººèº«ä¿éšªï¼å¹´é‡‘ä¿éšª",
                    "äººèº«ä¿éšªï¼å¥åº·ä¿éšª", "äººèº«ä¿éšªï¼å‚·å®³ä¿éšª", "äººèº«ä¿éšªï¼å…¶ä»–äººèº«ä¿éšª", "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·"
                ]
            }
        ],
        "default_chapter": "æŠ•ä¿å¯¦å‹™èˆ‡è¡ŒéŠ·"
    },
    "æŠ•è³‡å‹ä¿éšª": {
        "folder": "æŠ•è³‡å‹",
        "note_file": "ç­†è¨˜_æŠ•è³‡å‹.pdf",
        "col_opts": ["é¸é …1", "é¸é …2", "é¸é …3", "é¸é …4"], 
        "outputs": [
            {
                "filename": "æŠ•è³‡å‹_æ³•ä»¤è¦ç« .xlsx",
                "chapters": [
                    "æŠ•è³‡å‹ä¿éšªæ¦‚è«–", "æŠ•è³‡å‹ä¿éšªæ³•ä»¤ä»‹ç´¹", "é‡‘èé«”ç³»æ¦‚è¿°", "è­‰åˆ¸æŠ•è³‡ä¿¡è¨—åŠé¡§å•ä¹‹è¦ç¯„èˆ‡åˆ¶åº¦"
                ]
            },
            {
                "filename": "æŠ•è³‡å‹_æŠ•è³‡å¯¦å‹™.xlsx",
                "chapters": [
                    "è²¨å¹£æ™‚é–“åƒ¹å€¼", "å‚µåˆ¸è©•åƒ¹", "è­‰åˆ¸è©•åƒ¹", "é¢¨éšªã€å ±é…¬èˆ‡æŠ•è³‡çµ„åˆ",
                    "è³‡æœ¬è³‡ç”¢è¨‚åƒ¹æ¨¡å¼ã€ç¸¾æ•ˆ", "æŠ•è³‡å·¥å…·ç°¡ä»‹"
                ]
            }
        ],
        "default_chapter": "æŠ•è³‡å‹ä¿éšªæ¦‚è«–"
    },
    "å¤–å¹£ä¿å–®": {
        "folder": "å¤–å¹£",
        "note_file": "ç­†è¨˜_å¤–å¹£.pdf",
        "col_opts": ["é¸é …ä¸€", "é¸é …äºŒ", "é¸é …ä¸‰", "é¸é …å››"],
        "outputs": [
            {
                "filename": "å¤–å¹£.xlsx", 
                "chapters": [
                    "å£½éšªåŸºæœ¬æ¦‚å¿µ", "ä¿éšªæ¥­è¾¦ç†å¤–åŒ¯æ¥­å‹™ç®¡ç†è¾¦æ³•", "ç®¡ç†å¤–åŒ¯æ¢ä¾‹", "å¤–åŒ¯æ”¶æ”¯æˆ–äº¤æ˜“ç”³å ±è¾¦æ³•",
                    "ä¿éšªæ¥­è¾¦ç†åœ‹å¤–æŠ•è³‡ç®¡ç†è¾¦æ³•", "äººèº«ä¿éšªæ¥­è¾¦ç†ä»¥å¤–å¹£æ”¶ä»˜ä¹‹éæŠ•è³‡å‹äººèº«ä¿éšªæ¥­å‹™æ‡‰å…·å‚™è³‡æ ¼æ¢ä»¶åŠæ³¨æ„äº‹é …",
                    "æŠ•è³‡å‹ä¿éšªè§€å¿µ", "æŠ•è³‡å‹ä¿éšªå°ˆè¨­å¸³ç°¿ä¿ç®¡æ©Ÿæ§‹åŠæŠ•è³‡æ¨™çš„æ‡‰æ³¨æ„äº‹é …",
                    "éŠ·å”®æ‡‰æ³¨æ„äº‹é …", "æ–°å‹æ…‹äººèº«ä¿éšªå•†å“å¯©æŸ¥", "ä¿éšªæ¥­å„é¡ç›£æ§æªæ–½"
                ]
            }
        ],
        "default_chapter": "å£½éšªåŸºæœ¬æ¦‚å¿µ"
    }
}

COL_Q = "é¡Œç›®"

# ==========================================
# å·¥å…·é¡åˆ¥ (Client & Logic)
# ==========================================

class GeminiClient:
    def __init__(self, api_key):
        self.client = genai.Client(api_key=api_key)
        self.model_name = "gemini-2.5-flash"
        
    def generate(self, prompt, temperature=0.1):
        # å¢åŠ é‡è©¦æ¬¡æ•¸èˆ‡ç­‰å¾…æ™‚é–“ï¼Œé¿å… API 429 éŒ¯èª¤
        for attempt in range(5):
            try:
                response = self.client.models.generate_content(
                    model=self.model_name, contents=prompt,
                    config=types.GenerateContentConfig(temperature=temperature, response_mime_type="application/json")
                )
                return response.text.strip()
            except Exception as e:
                wait = (attempt + 1) * 5
                print(f"API Busy, retrying in {wait}s... Error: {e}")
                time.sleep(wait)
        return ""

class ChapterManager:
    def __init__(self, folder_name, pdf_filename, all_chapters, ai_client):
        self.pdf_path = f"{BASE_BANK_DIR}/{folder_name}/{pdf_filename}"
        self.all_chapters = all_chapters
        self.ai = ai_client
        self.full_note_text = ""
        self.chapter_keywords = {} 
        self._read_pdf_from_github()
        self._gen_keywords()

    def _read_pdf_from_github(self):
        data = gh.gh_download_bytes(self.pdf_path)
        if not data: return
        try:
            with pdfplumber.open(BytesIO(data)) as pdf:
                content = [p.extract_text() for p in pdf.pages if p.extract_text()]
                self.full_note_text = "\n".join(content)
        except: pass

    def _get_context(self, chapter):
        # ç°¡åŒ–ç‰ˆ Context æŠ“å–ï¼ŒåªæŠ“æœ€ç›¸é—œçš„ä¸€å°æ®µä»¥ç¯€çœ Token
        if not self.full_note_text: return ""
        idx = self.full_note_text.find(chapter)
        if idx != -1:
            return self.full_note_text[idx:idx+800]
        return ""

    def _gen_keywords(self):
        if not self.full_note_text:
            for ch in self.all_chapters: self.chapter_keywords[ch] = [ch]
            return

        progress_text = "AI æ­£åœ¨é–±è®€ç­†è¨˜å»ºç«‹ç´¢å¼• (åªéœ€åŸ·è¡Œä¸€æ¬¡)..."
        my_bar = st.progress(0, text=progress_text)
        
        # æ‰¹æ¬¡è™•ç†é—œéµå­—ç”Ÿæˆï¼Œä¸è¦ä¸€ç« ä¸€ç« å•
        prompt = (
            f"ä½ æ˜¯ä¿éšªè€ƒé¡Œåˆ†é¡å°ˆå®¶ã€‚è«‹é–±è®€ç­†è¨˜å¾Œï¼Œé‡å°ä¸‹åˆ—ç« ç¯€ï¼Œå„åˆ—å‡º 5 å€‹æœ€é—œéµçš„å°ˆæœ‰åè©ã€‚\n"
            f"ç« ç¯€åˆ—è¡¨ï¼š{self.all_chapters}\n"
            f"ç­†è¨˜å…§å®¹æ‘˜è¦ï¼š{self.full_note_text[:5000]}...\n\n"
            f"è«‹å›å‚³ JSON æ ¼å¼ï¼š{{ \"ç« ç¯€å\": [\"é—œéµå­—1\", \"é—œéµå­—2\"...] }}"
        )
        try:
            res = self.ai.generate(prompt)
            data = json.loads(res)
            # ç¢ºä¿æ‰€æœ‰ç« ç¯€éƒ½æœ‰ key
            for ch in self.all_chapters:
                self.chapter_keywords[ch] = data.get(ch, [ch])
        except:
            # å¤±æ•—å›é€€æ©Ÿåˆ¶
            for ch in self.all_chapters: self.chapter_keywords[ch] = [ch]
            
        my_bar.progress(1.0, text="ç´¢å¼•å»ºç«‹å®Œæˆï¼")
        time.sleep(1)
        my_bar.empty()

class SmartClassifier:
    def __init__(self, mgr, default_ch):
        self.mgr = mgr
        self.default_ch = default_ch
        self.chapters_str = "\n".join([f"- {c}" for c in mgr.all_chapters])

    def classify_batch(self, batch_data):
        """
        æ‰¹æ¬¡åˆ†é¡æ ¸å¿ƒé‚è¼¯
        batch_data: list of dict [{'id': 0, 'q': '...', 'opts': '...'}, ...]
        """
        results = {}
        ai_queue = []

        # 1. å…ˆç”¨é—œéµå­—å¿«é€Ÿç¯©é¸ (æœ¬åœ°é‹ç®—ï¼Œæ¥µå¿«)
        for item in batch_data:
            full_text = f"{item['q']} {item['opts']}"
            scores = Counter()
            for ch, kws in self.mgr.chapter_keywords.items():
                for kw in kws:
                    if kw in full_text: 
                        weight = 5 if kw == ch else 1
                        scores[ch] += weight
            
            best, val = scores.most_common(1)[0] if scores else (None, 0)
            
            if val >= 2:
                results[item['id']] = (best, "é—œéµå­—")
            else:
                ai_queue.append(item)

        # 2. å‰©ä¸‹çš„æ‰“åŒ…å• AI (æ¸›å°‘ API å‘¼å«æ¬¡æ•¸)
        if ai_queue:
            # é™åˆ¶ä¸€æ¬¡å• 10 é¡Œï¼Œé¿å… Token çˆ†æ‰
            prompt_items = []
            for item in ai_queue:
                prompt_items.append(f"ID {item['id']}:\né¡Œç›®: {item['q']}\né¸é …: {item['opts']}")
            
            prompt_str = "\n\n".join(prompt_items)
            prompt = (
                f"è«‹å°‡ä¸‹åˆ—é¡Œç›®åˆ†é¡åˆ°æœ€åˆé©çš„ç« ç¯€ã€‚å¯é¸ç« ç¯€ï¼š\n{self.mgr.all_chapters}\n\n"
                f"{prompt_str}\n\n"
                f"è«‹ç›´æ¥å›å‚³ JSON æ ¼å¼ï¼Œä¸è¦æœ‰Markdownæ¨™è¨˜ï¼š\n"
                f"[{{ \"id\": é¡Œè™Ÿ, \"chapter\": \"ç« ç¯€åç¨±\" }}, ...]"
            )
            
            try:
                # å‘¼å« AI
                res_text = self.mgr.ai.generate(prompt)
                # æ¸…ç†å¯èƒ½çš„å›å‚³é›œè¨Š
                res_text = res_text.replace("```json", "").replace("```", "")
                ai_results = json.loads(res_text)
                
                # è§£æå›å‚³
                for res in ai_results:
                    res_id = res.get('id')
                    raw_ch = res.get('chapter', self.default_ch)
                    
                    # æ¨¡ç³Šæ¯”å°ç« ç¯€åç¨± (é˜²æ­¢ AI å¯«éŒ¯å­—)
                    matches = difflib.get_close_matches(raw_ch, self.mgr.all_chapters, n=1, cutoff=0.4)
                    final_ch = matches[0] if matches else self.default_ch
                    
                    results[res_id] = (final_ch, "AIåˆ¤æ–·")
                    
            except Exception as e:
                print(f"Batch AI Failed: {e}")
                # å¤±æ•—æ™‚å…¨éƒ¨æ­¸ç‚ºé è¨­
                for item in ai_queue:
                    results[item['id']] = (self.default_ch, "é è¨­(APIå¤±æ•—)")

        return results

# ==========================================
# å°å¤–ä»‹é¢å‡½æ•¸
# ==========================================

@st.cache_resource(show_spinner=False)
def get_cached_manager(folder_name, note_filename, all_chapters_tuple):
    client = GeminiClient(GEMINI_API_KEY)
    return ChapterManager(folder_name, note_filename, list(all_chapters_tuple), client)

def process_uploaded_file(exam_type, uploaded_file):
    config = EXAM_CONFIGS.get(exam_type)
    if not config: return None

    all_chapters = []
    for output_conf in config['outputs']:
        all_chapters.extend(output_conf['chapters'])

    mgr = get_cached_manager(config['folder'], config['note_file'], tuple(all_chapters))
    classifier = SmartClassifier(mgr, config['default_chapter'])

    try:
        dfs = pd.read_excel(uploaded_file, sheet_name=None)
    except Exception as e:
        st.error(f"Excel è®€å–å¤±æ•—: {e}")
        return None

    final_results = []
    progress_bar = st.progress(0, text="æº–å‚™é–‹å§‹åˆ†é¡...")
    
    # é å…ˆè¨ˆç®—ç¸½é¡Œæ•¸ä»¥é¡¯ç¤ºé€²åº¦
    total_rows = sum(len(df) for df in dfs.values() if not df.empty and COL_Q in df.columns)
    processed_count = 0
    
    BATCH_SIZE = 10  # æ¯ 10 é¡Œå•ä¸€æ¬¡ AI

    for name, df in dfs.items():
        if df.empty or COL_Q not in df.columns: continue
        
        valid_opts = [c for c in config['col_opts'] if c in df.columns]
        
        # æº–å‚™æ‰¹æ¬¡è³‡æ–™å®¹å™¨
        batch_buffer = [] 
        rows_map = {} # ç”¨ id å°æ‡‰ row è³‡æ–™
        
        for idx, row in df.iterrows():
            q = str(row.get(COL_Q, "")).strip()
            if not q or q.lower() == "nan": continue
            
            opts_txt = " ".join([str(row.get(c, "")) for c in valid_opts])
            
            # çµ¦æ¯å€‹é¡Œç›®ä¸€å€‹å”¯ä¸€ ID (Sheetå_Index)
            unique_id = f"{name}_{idx}"
            item = {'id': unique_id, 'q': q, 'opts': opts_txt}
            
            batch_buffer.append(item)
            rows_map[unique_id] = row.to_dict()
            
            # ç•¶ Buffer æ»¿äº†ï¼Œæˆ–æ˜¯æœ€å¾Œä¸€é¡Œï¼Œå°±åŸ·è¡Œæ‰¹æ¬¡åˆ†é¡
            if len(batch_buffer) >= BATCH_SIZE or idx == len(df) - 1:
                # åŸ·è¡Œåˆ†é¡ï¼
                batch_results = classifier.classify_batch(batch_buffer)
                
                # å°‡çµæœå¯«å› row
                for item in batch_buffer:
                    res = batch_results.get(item['id'], (config['default_chapter'], "é è¨­"))
                    r = rows_map[item['id']]
                    r["AIåˆ†é¡ç« ç¯€"] = res[0]
                    r["åˆ†é¡ä¾†æº"] = res[1]
                    final_results.append(r)
                
                # æ›´æ–°é€²åº¦æ¢
                processed_count += len(batch_buffer)
                progress = min(processed_count / total_rows, 1.0)
                progress_bar.progress(progress, text=f"ğŸ”¥ é«˜é€Ÿåˆ†é¡ä¸­ï¼š{processed_count}/{total_rows} é¡Œ")
                
                # æ¸…ç©º Buffer
                batch_buffer = []
                
                # å¾®å°ä¼‘æ¯ï¼Œé¿å…å¤ªéé »ç¹
                time.sleep(2) # æ‰¹æ¬¡æ¨¡å¼ä¸‹ï¼Œé€™è£¡ä¼‘æ¯ 2 ç§’éå¸¸å®‰å…¨

    progress_bar.empty()
    return pd.DataFrame(final_results)

def save_merged_results(exam_type, new_classified_df):
    config = EXAM_CONFIGS.get(exam_type)
    base_gh_path = f"{BASE_BANK_DIR}/{config['folder']}"
    logs = []

    for out_conf in config['outputs']:
        filename = out_conf['filename']
        target_chs = out_conf['chapters']
        target_gh_path = f"{base_gh_path}/{filename}"

        sub_new = new_classified_df[new_classified_df["AIåˆ†é¡ç« ç¯€"].isin(target_chs)].copy()
        if sub_new.empty: continue

        existing_df = pd.DataFrame()
        old_file_bytes = gh.gh_download_bytes(target_gh_path)
        if old_file_bytes:
            try:
                xls = pd.read_excel(BytesIO(old_file_bytes), sheet_name=None)
                for sname, sdf in xls.items():
                    if "AIåˆ†é¡ç« ç¯€" not in sdf.columns: sdf["AIåˆ†é¡ç« ç¯€"] = sname
                    existing_df = pd.concat([existing_df, sdf], ignore_index=True)
            except: pass

        if not existing_df.empty:
            common = list(set(existing_df.columns) & set(sub_new.columns))
            if COL_Q in common:
                combined = pd.concat([existing_df, sub_new], ignore_index=True)
            else:
                combined = sub_new
        else:
            combined = sub_new
        
        before = len(combined)
        combined.drop_duplicates(subset=[COL_Q], keep='last', inplace=True)
        after = len(combined)
        
        logs.append(f"ğŸ“„ **{filename}**ï¼šæ–°å¢ {len(sub_new)} é¡Œï¼Œåˆä½µå¾Œå…± {after} é¡Œ (å·²å»é‡)ã€‚")

        mapper = {name: i for i, name in enumerate(target_chs)}
        combined["Sort"] = combined["AIåˆ†é¡ç« ç¯€"].map(mapper).fillna(999)
        combined = combined.sort_values("Sort")

        output = BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            for ch in target_chs:
                ch_df = combined[combined["AIåˆ†é¡ç« ç¯€"] == ch]
                if not ch_df.empty:
                    safe = ch.replace("/", "_")[:30]
                    ch_df.drop(columns=["Sort"], errors="ignore").to_excel(writer, sheet_name=safe, index=False)
        
        gh.gh_put_file(target_gh_path, output.getvalue(), f"Auto-Merge: {filename}")

    return logs